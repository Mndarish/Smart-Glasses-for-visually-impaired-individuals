{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uqXlAyEyS85D"
      },
      "source": [
        "/////////////////////////////sabkuch except tts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JfxgrQTUTI03",
        "outputId": "966ba264-145e-490b-cdce-a4dafb1c83ea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aVywi_wzTHxe",
        "outputId": "952e74dd-4c4c-4d4e-9875-fe0a69664337"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pyngrok in /usr/local/lib/python3.11/dist-packages (7.2.3)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.11/dist-packages (from pyngrok) (6.0.2)\n",
            "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n"
          ]
        }
      ],
      "source": [
        "!pip install pyngrok\n",
        "!ngrok config add-authtoken 2t7eHroGCkLrnrgTlZkzdWEuVLP_2CWMekhkpnc894q1eXwLA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "So1zBiFfTJoa",
        "outputId": "0617662f-c774-40a4-8cfe-409b75ab5264"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: flask in /usr/local/lib/python3.11/dist-packages (3.1.0)\n",
            "Requirement already satisfied: flask-cors in /usr/local/lib/python3.11/dist-packages (5.0.1)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (4.11.0.86)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: gtts in /usr/local/lib/python3.11/dist-packages (2.5.4)\n",
            "Collecting ultralytics\n",
            "  Downloading ultralytics-8.3.94-py3-none-any.whl.metadata (35 kB)\n",
            "Collecting deepface\n",
            "  Downloading deepface-0.0.93-py3-none-any.whl.metadata (30 kB)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (11.1.0)\n",
            "Requirement already satisfied: pyngrok in /usr/local/lib/python3.11/dist-packages (7.2.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: Werkzeug>=3.1 in /usr/local/lib/python3.11/dist-packages (from flask) (3.1.3)\n",
            "Requirement already satisfied: Jinja2>=3.1.2 in /usr/local/lib/python3.11/dist-packages (from flask) (3.1.6)\n",
            "Requirement already satisfied: itsdangerous>=2.2 in /usr/local/lib/python3.11/dist-packages (from flask) (2.2.0)\n",
            "Requirement already satisfied: click>=8.1.3 in /usr/local/lib/python3.11/dist-packages (from flask) (8.1.8)\n",
            "Requirement already satisfied: blinker>=1.9 in /usr/local/lib/python3.11/dist-packages (from flask) (1.9.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.11/dist-packages (from gtts) (2.32.3)\n",
            "Requirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (3.10.0)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (6.0.2)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (1.14.1)\n",
            "Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (0.21.0+cu124)\n",
            "Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (4.67.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from ultralytics) (5.9.5)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.11/dist-packages (from ultralytics) (9.0.0)\n",
            "Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.2.2)\n",
            "Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (0.13.2)\n",
            "Collecting ultralytics-thop>=2.0.0 (from ultralytics)\n",
            "  Downloading ultralytics_thop-2.0.14-py3-none-any.whl.metadata (9.4 kB)\n",
            "Requirement already satisfied: gdown>=3.10.1 in /usr/local/lib/python3.11/dist-packages (from deepface) (5.2.0)\n",
            "Requirement already satisfied: tensorflow>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from deepface) (2.18.0)\n",
            "Requirement already satisfied: keras>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from deepface) (3.8.0)\n",
            "Collecting mtcnn>=0.1.0 (from deepface)\n",
            "  Downloading mtcnn-1.0.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "Collecting retina-face>=0.0.1 (from deepface)\n",
            "  Downloading retina_face-0.0.17-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting fire>=0.4.0 (from deepface)\n",
            "  Downloading fire-0.7.0.tar.gz (87 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.2/87.2 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting gunicorn>=20.1.0 (from deepface)\n",
            "  Downloading gunicorn-23.0.0-py3-none-any.whl.metadata (4.4 kB)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.11/dist-packages (from fire>=0.4.0->deepface) (2.5.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from gdown>=3.10.1->deepface) (4.13.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from gunicorn>=20.1.0->deepface) (24.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from Jinja2>=3.1.2->flask) (3.0.2)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from keras>=2.2.0->deepface) (1.4.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=2.2.0->deepface) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=2.2.0->deepface) (0.0.8)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.11/dist-packages (from keras>=2.2.0->deepface) (3.13.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=2.2.0->deepface) (0.14.1)\n",
            "Requirement already satisfied: ml-dtypes in /usr/local/lib/python3.11/dist-packages (from keras>=2.2.0->deepface) (0.4.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.8)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.8.2)\n",
            "Requirement already satisfied: joblib>=1.4.2 in /usr/local/lib/python3.11/dist-packages (from mtcnn>=0.1.0->deepface) (1.4.2)\n",
            "Collecting lz4>=4.3.3 (from mtcnn>=0.1.0->deepface)\n",
            "  Downloading lz4-4.4.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.4->ultralytics) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.4->ultralytics) (2025.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->gtts) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->gtts) (2.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->gtts) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->gtts) (2025.1.31)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=1.9.0->deepface) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=1.9.0->deepface) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=1.9.0->deepface) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=1.9.0->deepface) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=1.9.0->deepface) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=1.9.0->deepface) (3.4.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=1.9.0->deepface) (5.29.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow>=1.9.0->deepface) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=1.9.0->deepface) (1.17.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=1.9.0->deepface) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=1.9.0->deepface) (1.71.0)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=1.9.0->deepface) (2.18.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=1.9.0->deepface) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow>=1.9.0->deepface) (0.45.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow>=1.9.0->deepface) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow>=1.9.0->deepface) (0.7.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->gdown>=3.10.1->deepface) (2.6)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown>=3.10.1->deepface) (1.7.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=2.2.0->deepface) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=2.2.0->deepface) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=2.2.0->deepface) (0.1.2)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m60.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m33.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m37.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m53.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ultralytics-8.3.94-py3-none-any.whl (949 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m949.8/949.8 kB\u001b[0m \u001b[31m38.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading deepface-0.0.93-py3-none-any.whl (108 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m108.6/108.6 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gunicorn-23.0.0-py3-none-any.whl (85 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.0/85.0 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mtcnn-1.0.0-py3-none-any.whl (1.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m53.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading retina_face-0.0.17-py3-none-any.whl (25 kB)\n",
            "Downloading ultralytics_thop-2.0.14-py3-none-any.whl (26 kB)\n",
            "Downloading lz4-4.4.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m43.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: fire\n",
            "  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fire: filename=fire-0.7.0-py3-none-any.whl size=114249 sha256=331d32a3670d9a7c59386513f261cc4c6cbca0799ada6e1cace304a04773bf42\n",
            "  Stored in directory: /root/.cache/pip/wheels/46/54/24/1624fd5b8674eb1188623f7e8e17cdf7c0f6c24b609dfb8a89\n",
            "Successfully built fire\n",
            "Installing collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, lz4, gunicorn, fire, nvidia-cusparse-cu12, nvidia-cudnn-cu12, mtcnn, nvidia-cusolver-cu12, ultralytics-thop, retina-face, ultralytics, deepface\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed deepface-0.0.93 fire-0.7.0 gunicorn-23.0.0 lz4-4.4.3 mtcnn-1.0.0 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 retina-face-0.0.17 ultralytics-8.3.94 ultralytics-thop-2.0.14\n"
          ]
        }
      ],
      "source": [
        "!pip install flask flask-cors opencv-python torch gtts ultralytics deepface pillow pyngrok numpy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zcZfjfyXUtET",
        "outputId": "239c3712-7040-42f9-b849-9ea05aed90ad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting mediapipe\n",
            "  Downloading mediapipe-0.10.21-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from mediapipe) (1.4.0)\n",
            "Requirement already satisfied: attrs>=19.1.0 in /usr/local/lib/python3.11/dist-packages (from mediapipe) (25.3.0)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.11/dist-packages (from mediapipe) (25.2.10)\n",
            "Requirement already satisfied: jax in /usr/local/lib/python3.11/dist-packages (from mediapipe) (0.5.2)\n",
            "Requirement already satisfied: jaxlib in /usr/local/lib/python3.11/dist-packages (from mediapipe) (0.5.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from mediapipe) (3.10.0)\n",
            "Collecting numpy<2 (from mediapipe)\n",
            "  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: opencv-contrib-python in /usr/local/lib/python3.11/dist-packages (from mediapipe) (4.11.0.86)\n",
            "Collecting protobuf<5,>=4.25.3 (from mediapipe)\n",
            "  Downloading protobuf-4.25.6-cp37-abi3-manylinux2014_x86_64.whl.metadata (541 bytes)\n",
            "Collecting sounddevice>=0.4.4 (from mediapipe)\n",
            "  Downloading sounddevice-0.5.1-py3-none-any.whl.metadata (1.4 kB)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (from mediapipe) (0.2.0)\n",
            "Requirement already satisfied: CFFI>=1.0 in /usr/local/lib/python3.11/dist-packages (from sounddevice>=0.4.4->mediapipe) (1.17.1)\n",
            "Requirement already satisfied: ml_dtypes>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from jax->mediapipe) (0.4.1)\n",
            "Requirement already satisfied: opt_einsum in /usr/local/lib/python3.11/dist-packages (from jax->mediapipe) (3.4.0)\n",
            "Requirement already satisfied: scipy>=1.11.1 in /usr/local/lib/python3.11/dist-packages (from jax->mediapipe) (1.14.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (2.8.2)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe) (2.22)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib->mediapipe) (1.17.0)\n",
            "Downloading mediapipe-0.10.21-cp311-cp311-manylinux_2_28_x86_64.whl (35.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.6/35.6 MB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m79.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading protobuf-4.25.6-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.6/294.6 kB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sounddevice-0.5.1-py3-none-any.whl (32 kB)\n",
            "Installing collected packages: protobuf, numpy, sounddevice, mediapipe\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 5.29.3\n",
            "    Uninstalling protobuf-5.29.3:\n",
            "      Successfully uninstalled protobuf-5.29.3\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "grpcio-status 1.71.0 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 4.25.6 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed mediapipe-0.10.21 numpy-1.26.4 protobuf-4.25.6 sounddevice-0.5.1\n"
          ]
        }
      ],
      "source": [
        "!pip install mediapipe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "meK6EbT_S_xa",
        "outputId": "b79fae52-43a4-4adc-a065-a5b0170886f0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolov8n.pt to 'yolov8n.pt'...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 6.25M/6.25M [00:00<00:00, 70.0MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Public URL: https://be2d-104-196-48-117.ngrok-free.app\n",
            " * Serving Flask app '__main__'\n",
            " * Debug mode: off\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
            " * Running on http://127.0.0.1:5000\n",
            "INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n",
            "INFO:werkzeug:127.0.0.1 - - [20/Mar/2025 12:55:55] \"POST /detect HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [20/Mar/2025 12:55:59] \"POST /detect HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [20/Mar/2025 12:56:02] \"POST /detect HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [20/Mar/2025 12:56:04] \"POST /detect HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [20/Mar/2025 12:56:07] \"POST /detect HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [20/Mar/2025 12:56:10] \"POST /detect HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [20/Mar/2025 12:56:13] \"POST /detect HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [20/Mar/2025 12:56:16] \"POST /detect HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [20/Mar/2025 12:56:18] \"POST /detect HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [20/Mar/2025 12:56:21] \"POST /detect HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [20/Mar/2025 12:56:24] \"POST /detect HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [20/Mar/2025 12:56:27] \"POST /detect HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [20/Mar/2025 12:56:30] \"POST /detect HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [20/Mar/2025 12:56:33] \"POST /detect HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [20/Mar/2025 12:56:36] \"POST /detect HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [20/Mar/2025 12:56:39] \"POST /detect HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [20/Mar/2025 12:56:42] \"POST /detect HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "0: 480x640 2 persons, 1 umbrella, 467.3ms\n",
            "Speed: 14.1ms preprocess, 467.3ms inference, 45.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 2 persons, 1 surfboard, 375.2ms\n",
            "Speed: 22.3ms preprocess, 375.2ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 (no detections), 309.9ms\n",
            "Speed: 6.7ms preprocess, 309.9ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
            "0: 480x640 (no detections), 231.6ms\n",
            "Speed: 4.7ms preprocess, 231.6ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "25-03-20 12:56:53 - facial_expression_model_weights.h5 will be downloaded...\n",
            "25-03-20 12:56:53 - facial_expression_model_weights.h5 will be downloaded...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "Downloading...\n",
            "From: https://github.com/serengil/deepface_models/releases/download/v1.0/facial_expression_model_weights.h5\n",
            "To: /root/.deepface/weights/facial_expression_model_weights.h5\n",
            "From: https://github.com/serengil/deepface_models/releases/download/v1.0/facial_expression_model_weights.h5\n",
            "To: /root/.deepface/weights/facial_expression_model_weights.h5\n",
            "  0%|          | 0.00/5.98M [00:00<?, ?B/s]\n",
            "100%|██████████| 5.98M/5.98M [00:00<00:00, 62.7MB/s]\n",
            "100%|██████████| 5.98M/5.98M [00:00<00:00, 61.4MB/s]\n",
            "INFO:werkzeug:127.0.0.1 - - [20/Mar/2025 12:56:54] \"POST /detect HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [20/Mar/2025 12:56:54] \"POST /detect HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [20/Mar/2025 12:57:03] \"POST /detect HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [20/Mar/2025 12:57:07] \"POST /detect HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "0: 480x640 3 persons, 236.2ms\n",
            "Speed: 5.5ms preprocess, 236.2ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 (no detections), 155.4ms\n",
            "Speed: 3.5ms preprocess, 155.4ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [20/Mar/2025 12:57:11] \"POST /detect HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "0: 480x640 3 persons, 175.2ms\n",
            "Speed: 5.9ms preprocess, 175.2ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 (no detections), 171.0ms\n",
            "Speed: 2.9ms preprocess, 171.0ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [20/Mar/2025 12:57:19] \"POST /detect HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "0: 480x640 4 persons, 178.7ms\n",
            "Speed: 6.5ms preprocess, 178.7ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 (no detections), 144.7ms\n",
            "Speed: 3.3ms preprocess, 144.7ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [20/Mar/2025 12:57:22] \"POST /detect HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "0: 480x640 3 persons, 313.2ms\n",
            "Speed: 4.3ms preprocess, 313.2ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 (no detections), 242.5ms\n",
            "Speed: 2.9ms preprocess, 242.5ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [20/Mar/2025 12:57:27] \"POST /detect HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "0: 480x640 2 persons, 204.2ms\n",
            "Speed: 5.5ms preprocess, 204.2ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 (no detections), 164.0ms\n",
            "Speed: 3.6ms preprocess, 164.0ms inference, 0.5ms postprocess per image at shape (1, 3, 480, 640)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [20/Mar/2025 12:57:34] \"POST /detect HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "0: 480x640 1 person, 273.9ms\n",
            "Speed: 3.1ms preprocess, 273.9ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 (no detections), 265.6ms\n",
            "Speed: 2.9ms preprocess, 265.6ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [20/Mar/2025 12:57:39] \"POST /detect HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [20/Mar/2025 12:57:42] \"POST /detect HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [20/Mar/2025 12:57:44] \"POST /detect HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [20/Mar/2025 12:57:47] \"POST /detect HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [20/Mar/2025 12:57:50] \"POST /detect HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [20/Mar/2025 12:57:53] \"POST /detect HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [20/Mar/2025 12:57:56] \"POST /detect HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [20/Mar/2025 12:57:58] \"POST /detect HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [20/Mar/2025 12:58:01] \"POST /detect HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [20/Mar/2025 12:58:03] \"POST /detect HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [20/Mar/2025 12:58:06] \"POST /detect HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [20/Mar/2025 12:58:09] \"POST /detect HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [20/Mar/2025 12:58:12] \"POST /detect HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [20/Mar/2025 12:58:14] \"POST /detect HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [20/Mar/2025 12:58:17] \"POST /detect HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [20/Mar/2025 12:58:19] \"POST /detect HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [20/Mar/2025 12:58:22] \"POST /detect HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [20/Mar/2025 12:58:24] \"POST /detect HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [20/Mar/2025 12:58:27] \"POST /detect HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [20/Mar/2025 12:58:30] \"POST /detect HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [20/Mar/2025 12:58:32] \"POST /detect HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [20/Mar/2025 12:58:35] \"POST /detect HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [20/Mar/2025 12:58:37] \"POST /detect HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [20/Mar/2025 12:58:40] \"POST /detect HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [20/Mar/2025 12:58:43] \"POST /detect HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [20/Mar/2025 12:58:45] \"POST /detect HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [20/Mar/2025 12:58:48] \"POST /detect HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [20/Mar/2025 12:58:50] \"POST /detect HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [20/Mar/2025 12:58:53] \"POST /detect HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [20/Mar/2025 12:58:56] \"POST /detect HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [20/Mar/2025 13:00:43] \"POST /detect HTTP/1.1\" 200 -\n"
          ]
        }
      ],
      "source": [
        "from flask import Flask, request, jsonify\n",
        "from flask_cors import CORS\n",
        "import cv2\n",
        "import numpy as np\n",
        "import torch\n",
        "from gtts import gTTS\n",
        "\n",
        "import time\n",
        "from threading import Thread\n",
        "from ultralytics import YOLO\n",
        "from deepface import DeepFace\n",
        "from io import BytesIO\n",
        "from PIL import Image\n",
        "import base64\n",
        "from pyngrok import ngrok\n",
        "import tempfile\n",
        "import mediapipe as mp\n",
        "\n",
        "# Initialize Flask app\n",
        "app = Flask(__name__)\n",
        "CORS(app)\n",
        "\n",
        "# Load models\n",
        "yolo_model = YOLO(\"yolov8n.pt\")  # General object detection\n",
        "currency_model = YOLO(\"/content/drive/MyDrive/best.pt\")  # Custom-trained currency detection\n",
        "\n",
        "# Initialize MediaPipe Hands\n",
        "mp_hands = mp.solutions.hands\n",
        "hands = mp_hands.Hands(min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
        "mp_draw = mp.solutions.drawing_utils\n",
        "\n",
        "# Start ngrok\n",
        "ngrok_tunnel = ngrok.connect(5000)\n",
        "print(f\"Public URL: {ngrok_tunnel.public_url}\")\n",
        "\n",
        "# Track last spoken time\n",
        "last_spoken_time = time.time()\n",
        "\n",
        "# YOLO processing control\n",
        "yolo_emotion_enabled = False\n",
        "\n",
        "def count_fingers(hand_landmarks):\n",
        "    \"\"\"Count the number of extended fingers.\"\"\"\n",
        "    finger_tips = [4, 8, 12, 16, 20]\n",
        "    extended_fingers = sum(\n",
        "        1 for tip in finger_tips if hand_landmarks.landmark[tip].y < hand_landmarks.landmark[tip - 2].y\n",
        "    )\n",
        "    return extended_fingers\n",
        "\n",
        "def generate_audio(text):\n",
        "    \"\"\"Generate speech audio using gTTS and return Base64-encoded MP3.\"\"\"\n",
        "    tts = gTTS(text=text, lang='en')\n",
        "\n",
        "    with tempfile.NamedTemporaryFile(delete=True, suffix=\".mp3\") as temp_audio:\n",
        "        tts.save(temp_audio.name)\n",
        "\n",
        "        # Read and encode the audio file\n",
        "        with open(temp_audio.name, \"rb\") as f:\n",
        "            audio_base64 = base64.b64encode(f.read()).decode(\"utf-8\")\n",
        "\n",
        "    return audio_base64\n",
        "\n",
        "@app.route('/detect', methods=['POST'])\n",
        "def detect_objects_and_emotions():\n",
        "    global last_spoken_time, yolo_emotion_enabled\n",
        "\n",
        "    try:\n",
        "        # Receive image\n",
        "        img_data = request.json['image']\n",
        "        img_bytes = base64.b64decode(img_data)\n",
        "        img = Image.open(BytesIO(img_bytes)).convert(\"RGB\")\n",
        "\n",
        "        # Convert to OpenCV format\n",
        "        img_np = np.array(img)\n",
        "        img_cv = cv2.cvtColor(img_np, cv2.COLOR_RGB2BGR)\n",
        "\n",
        "        # Gesture Recognition (always runs)\n",
        "        rgb_img = cv2.cvtColor(img_cv, cv2.COLOR_BGR2RGB)\n",
        "        result = hands.process(rgb_img)\n",
        "\n",
        "        gesture_text = \"No Change\"\n",
        "        if result.multi_hand_landmarks:\n",
        "            for hand_landmarks in result.multi_hand_landmarks:\n",
        "                mp_draw.draw_landmarks(img_cv, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
        "                fingers = count_fingers(hand_landmarks)\n",
        "\n",
        "                if fingers == 1:\n",
        "                    yolo_emotion_enabled = True\n",
        "                    gesture_text = \"Process Enabled\"\n",
        "                elif fingers == 3:\n",
        "                    yolo_emotion_enabled = False\n",
        "                    gesture_text = \"Process Disabled\"\n",
        "\n",
        "                cv2.putText(img_cv, f\"Gesture: {gesture_text}\", (50, 100), cv2.FONT_HERSHEY_SIMPLEX,\n",
        "                            1, (0, 255, 0), 2, cv2.LINE_AA)\n",
        "\n",
        "        detected_objects = []\n",
        "        emotion_text = \"\"\n",
        "        speech_text = \"\"\n",
        "        audio_base64 = \"\"\n",
        "\n",
        "        # **Always Send Frames, but Process Only When Enabled**\n",
        "        if yolo_emotion_enabled:\n",
        "            # Run YOLO detection\n",
        "            results = yolo_model(img_cv)[0]\n",
        "            currency_results = currency_model(img_cv)[0]\n",
        "\n",
        "            # Process object detections\n",
        "            for box in results.boxes:\n",
        "                x1, y1, x2, y2 = map(int, box.xyxy[0])\n",
        "                conf = float(box.conf[0])\n",
        "                cls = int(box.cls[0])\n",
        "                label = yolo_model.names[cls]\n",
        "                detected_objects.append(label)\n",
        "\n",
        "                cv2.rectangle(img_cv, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
        "                cv2.putText(img_cv, label, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
        "\n",
        "            # Process currency detections\n",
        "            for box in currency_results.boxes:\n",
        "                x1, y1, x2, y2 = map(int, box.xyxy[0])\n",
        "                conf = float(box.conf[0])\n",
        "                label = \"Currency\"\n",
        "                detected_objects.append(label)\n",
        "\n",
        "                cv2.rectangle(img_cv, (x1, y1), (x2, y2), (255, 0, 0), 2)\n",
        "                cv2.putText(img_cv, label, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 2)\n",
        "\n",
        "            # Emotion detection\n",
        "            result = DeepFace.analyze(img_cv, actions=['emotion'], enforce_detection=False, detector_backend='opencv')[0]\n",
        "            emotion_text = result['dominant_emotion']\n",
        "\n",
        "            # Display emotion on image\n",
        "            cv2.putText(img_cv, emotion_text, (20, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
        "\n",
        "# Construct speech text\n",
        "            speech_text = \"\"\n",
        "\n",
        "# Check if a person is detected\n",
        "            person_detected = any(obj.lower() == \"person\" for obj in detected_objects)\n",
        "\n",
        "            if person_detected and emotion_text:\n",
        "                  speech_text += f\"A person detected with {emotion_text} emotion. \"\n",
        "\n",
        "# Announce other objects\n",
        "            for obj in detected_objects:\n",
        "              if obj.lower() == \"currency\":\n",
        "                 speech_text += \"Currency detected. \"\n",
        "              elif obj.lower() != \"person\":  # Exclude 'person' since it's handled above\n",
        "                 speech_text += f\"A {obj} detected. \"\n",
        "\n",
        "# If no specific objects are detected, provide a general message\n",
        "            if not speech_text:\n",
        "             speech_text = \"An object is detected.\"\n",
        "\n",
        "\n",
        "            # Speak only if 15 seconds passed\n",
        "        # Generate audio only if 15 seconds have passed\n",
        "        audio_base64 = \"\"\n",
        "        current_time = time.time()\n",
        "        if speech_text and current_time - last_spoken_time >= 15:\n",
        "            last_spoken_time = current_time\n",
        "            audio_base64 = generate_audio(speech_text)\n",
        "\n",
        "        # Encode and Always Send Processed Image\n",
        "        _, buffer = cv2.imencode('.jpg', img_cv)\n",
        "        img_encoded = base64.b64encode(buffer).decode('utf-8')\n",
        "\n",
        "        return jsonify({\n",
        "            \"processed_image\": img_encoded,\n",
        "            \"emotion\": emotion_text,\n",
        "            \"gesture\": gesture_text,\n",
        "            \"audio\": audio_base64\n",
        "        })\n",
        "\n",
        "    except Exception as e:\n",
        "        return jsonify({\"error\": str(e)})\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    app.run(port=5000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oQebcxvtoDix"
      },
      "source": [
        "//////////////////////ultimate tts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FxWFPZotSwDh",
        "outputId": "21436fed-e62a-4c6f-8407-d3808578ef31"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pyngrok\n",
            "  Downloading pyngrok-7.2.3-py3-none-any.whl.metadata (8.7 kB)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.11/dist-packages (from pyngrok) (6.0.2)\n",
            "Downloading pyngrok-7.2.3-py3-none-any.whl (23 kB)\n",
            "Installing collected packages: pyngrok\n",
            "Successfully installed pyngrok-7.2.3\n",
            "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n"
          ]
        }
      ],
      "source": [
        "!pip install pyngrok\n",
        "!ngrok config add-authtoken 2t7eHroGCkLrnrgTlZkzdWEuVLP_2CWMekhkpnc894q1eXwLA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qRpRG26vo5vl",
        "outputId": "60e22658-bdb2-4c77-eb3f-db923ddf1a84"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: Flask in /usr/local/lib/python3.11/dist-packages (3.1.0)\n",
            "Collecting flask-ngrok\n",
            "  Downloading flask_ngrok-0.0.25-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: Werkzeug>=3.1 in /usr/local/lib/python3.11/dist-packages (from Flask) (3.1.3)\n",
            "Requirement already satisfied: Jinja2>=3.1.2 in /usr/local/lib/python3.11/dist-packages (from Flask) (3.1.6)\n",
            "Requirement already satisfied: itsdangerous>=2.2 in /usr/local/lib/python3.11/dist-packages (from Flask) (2.2.0)\n",
            "Requirement already satisfied: click>=8.1.3 in /usr/local/lib/python3.11/dist-packages (from Flask) (8.1.8)\n",
            "Requirement already satisfied: blinker>=1.9 in /usr/local/lib/python3.11/dist-packages (from Flask) (1.9.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from flask-ngrok) (2.32.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from Jinja2>=3.1.2->Flask) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->flask-ngrok) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->flask-ngrok) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->flask-ngrok) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->flask-ngrok) (2025.1.31)\n",
            "Downloading flask_ngrok-0.0.25-py3-none-any.whl (3.1 kB)\n",
            "Installing collected packages: flask-ngrok\n",
            "Successfully installed flask-ngrok-0.0.25\n",
            "Collecting pytesseract\n",
            "  Downloading pytesseract-0.3.13-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.11/dist-packages (from pytesseract) (24.2)\n",
            "Requirement already satisfied: Pillow>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from pytesseract) (11.1.0)\n",
            "Downloading pytesseract-0.3.13-py3-none-any.whl (14 kB)\n",
            "Installing collected packages: pytesseract\n",
            "Successfully installed pytesseract-0.3.13\n",
            "Collecting langdetect\n",
            "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from langdetect) (1.17.0)\n",
            "Building wheels for collected packages: langdetect\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993222 sha256=35a19dc60af10842290c7caf58c46a750ba436c179ae3f0fe926f6a21fe12736\n",
            "  Stored in directory: /root/.cache/pip/wheels/0a/f2/b2/e5ca405801e05eb7c8ed5b3b4bcf1fcabcd6272c167640072e\n",
            "Successfully built langdetect\n",
            "Installing collected packages: langdetect\n",
            "Successfully installed langdetect-1.0.9\n",
            "Collecting gTTS\n",
            "  Downloading gTTS-2.5.4-py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.11/dist-packages (from gTTS) (2.32.3)\n",
            "Requirement already satisfied: click<8.2,>=7.1 in /usr/local/lib/python3.11/dist-packages (from gTTS) (8.1.8)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->gTTS) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->gTTS) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->gTTS) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->gTTS) (2025.1.31)\n",
            "Downloading gTTS-2.5.4-py3-none-any.whl (29 kB)\n",
            "Installing collected packages: gTTS\n",
            "Successfully installed gTTS-2.5.4\n",
            "Collecting googletrans==4.0.0-rc1\n",
            "  Downloading googletrans-4.0.0rc1.tar.gz (20 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting httpx==0.13.3 (from googletrans==4.0.0-rc1)\n",
            "  Downloading httpx-0.13.3-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (2025.1.31)\n",
            "Collecting hstspreload (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading hstspreload-2025.1.1-py3-none-any.whl.metadata (2.1 kB)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (1.3.1)\n",
            "Collecting chardet==3.* (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading chardet-3.0.4-py2.py3-none-any.whl.metadata (3.2 kB)\n",
            "Collecting idna==2.* (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading idna-2.10-py2.py3-none-any.whl.metadata (9.1 kB)\n",
            "Collecting rfc3986<2,>=1.3 (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading rfc3986-1.5.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting httpcore==0.9.* (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading httpcore-0.9.1-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting h11<0.10,>=0.8 (from httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading h11-0.9.0-py2.py3-none-any.whl.metadata (8.1 kB)\n",
            "Collecting h2==3.* (from httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading h2-3.2.0-py2.py3-none-any.whl.metadata (32 kB)\n",
            "Collecting hyperframe<6,>=5.2.0 (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading hyperframe-5.2.0-py2.py3-none-any.whl.metadata (7.2 kB)\n",
            "Collecting hpack<4,>=3.0 (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading hpack-3.0.0-py2.py3-none-any.whl.metadata (7.0 kB)\n",
            "Downloading httpx-0.13.3-py3-none-any.whl (55 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.1/55.1 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading chardet-3.0.4-py2.py3-none-any.whl (133 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.4/133.4 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpcore-0.9.1-py3-none-any.whl (42 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading idna-2.10-py2.py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading h2-3.2.0-py2.py3-none-any.whl (65 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.0/65.0 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rfc3986-1.5.0-py2.py3-none-any.whl (31 kB)\n",
            "Downloading hstspreload-2025.1.1-py3-none-any.whl (1.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m29.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading h11-0.9.0-py2.py3-none-any.whl (53 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.6/53.6 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading hpack-3.0.0-py2.py3-none-any.whl (38 kB)\n",
            "Downloading hyperframe-5.2.0-py2.py3-none-any.whl (12 kB)\n",
            "Building wheels for collected packages: googletrans\n",
            "  Building wheel for googletrans (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for googletrans: filename=googletrans-4.0.0rc1-py3-none-any.whl size=17397 sha256=50ff25a62273d011bd4cb830f0cc78cbf8bc1963eff0901c1aa649932f66aa2e\n",
            "  Stored in directory: /root/.cache/pip/wheels/39/17/6f/66a045ea3d168826074691b4b787b8f324d3f646d755443fda\n",
            "Successfully built googletrans\n",
            "Installing collected packages: rfc3986, hyperframe, hpack, h11, chardet, idna, hstspreload, h2, httpcore, httpx, googletrans\n",
            "  Attempting uninstall: hyperframe\n",
            "    Found existing installation: hyperframe 6.1.0\n",
            "    Uninstalling hyperframe-6.1.0:\n",
            "      Successfully uninstalled hyperframe-6.1.0\n",
            "  Attempting uninstall: hpack\n",
            "    Found existing installation: hpack 4.1.0\n",
            "    Uninstalling hpack-4.1.0:\n",
            "      Successfully uninstalled hpack-4.1.0\n",
            "  Attempting uninstall: h11\n",
            "    Found existing installation: h11 0.14.0\n",
            "    Uninstalling h11-0.14.0:\n",
            "      Successfully uninstalled h11-0.14.0\n",
            "  Attempting uninstall: chardet\n",
            "    Found existing installation: chardet 5.2.0\n",
            "    Uninstalling chardet-5.2.0:\n",
            "      Successfully uninstalled chardet-5.2.0\n",
            "  Attempting uninstall: idna\n",
            "    Found existing installation: idna 3.10\n",
            "    Uninstalling idna-3.10:\n",
            "      Successfully uninstalled idna-3.10\n",
            "  Attempting uninstall: h2\n",
            "    Found existing installation: h2 4.2.0\n",
            "    Uninstalling h2-4.2.0:\n",
            "      Successfully uninstalled h2-4.2.0\n",
            "  Attempting uninstall: httpcore\n",
            "    Found existing installation: httpcore 1.0.7\n",
            "    Uninstalling httpcore-1.0.7:\n",
            "      Successfully uninstalled httpcore-1.0.7\n",
            "  Attempting uninstall: httpx\n",
            "    Found existing installation: httpx 0.28.1\n",
            "    Uninstalling httpx-0.28.1:\n",
            "      Successfully uninstalled httpx-0.28.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "openai 1.66.3 requires httpx<1,>=0.23.0, but you have httpx 0.13.3 which is incompatible.\n",
            "google-genai 1.5.0 requires httpx<1.0.0dev,>=0.28.1, but you have httpx 0.13.3 which is incompatible.\n",
            "langsmith 0.3.15 requires httpx<1,>=0.23.0, but you have httpx 0.13.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed chardet-3.0.4 googletrans-4.0.0rc1 h11-0.9.0 h2-3.2.0 hpack-3.0.0 hstspreload-2025.1.1 httpcore-0.9.1 httpx-0.13.3 hyperframe-5.2.0 idna-2.10 rfc3986-1.5.0\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (11.1.0)\n",
            "Requirement already satisfied: pygame in /usr/local/lib/python3.11/dist-packages (2.6.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install Flask flask-ngrok\n",
        "!pip install pytesseract\n",
        "!pip install langdetect\n",
        "!pip install gTTS\n",
        "!pip install googletrans==4.0.0-rc1\n",
        "!pip install pillow\n",
        "!pip install pygame\n",
        "!pip install flask-cors\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FCFFhITiTrmC",
        "outputId": "ea048a78-ecc9-4562-cb3a-b0061a6f87c1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting flask-cors\n",
            "  Downloading flask_cors-5.0.1-py3-none-any.whl.metadata (961 bytes)\n",
            "Requirement already satisfied: flask>=0.9 in /usr/local/lib/python3.11/dist-packages (from flask-cors) (3.1.0)\n",
            "Requirement already satisfied: Werkzeug>=0.7 in /usr/local/lib/python3.11/dist-packages (from flask-cors) (3.1.3)\n",
            "Requirement already satisfied: Jinja2>=3.1.2 in /usr/local/lib/python3.11/dist-packages (from flask>=0.9->flask-cors) (3.1.6)\n",
            "Requirement already satisfied: itsdangerous>=2.2 in /usr/local/lib/python3.11/dist-packages (from flask>=0.9->flask-cors) (2.2.0)\n",
            "Requirement already satisfied: click>=8.1.3 in /usr/local/lib/python3.11/dist-packages (from flask>=0.9->flask-cors) (8.1.8)\n",
            "Requirement already satisfied: blinker>=1.9 in /usr/local/lib/python3.11/dist-packages (from flask>=0.9->flask-cors) (1.9.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from Werkzeug>=0.7->flask-cors) (3.0.2)\n",
            "Downloading flask_cors-5.0.1-py3-none-any.whl (11 kB)\n",
            "Installing collected packages: flask-cors\n",
            "Successfully installed flask-cors-5.0.1\n"
          ]
        }
      ],
      "source": [
        "!pip install flask-cors\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6C9lmY7HoFOk",
        "outputId": "41165852-35be-4abd-b889-d7307a3674d4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Public URL: https://bc1a-35-243-180-6.ngrok-free.app\n",
            " * Serving Flask app '__main__'\n",
            " * Debug mode: off\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
            " * Running on http://127.0.0.1:5000\n",
            "INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "📌 Detected Language: unknown\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [21/Mar/2025 15:30:28] \"\u001b[31m\u001b[1mPOST /process_frame HTTP/1.1\u001b[0m\" 400 -\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "📌 Detected Language: en\n",
            "\n",
            "📝 Extracted Text:\n",
            "wey eee ge\n",
            "\n",
            "he rop of the TVinthe sitting roam, cycing the Wo men\n",
            "\n",
            ":_ they talked, until Satoru Filly fe Femi fhe bottle « of We : “sf\n",
            "sym the table.” ee ee ee i gate!\n",
            "\n",
            "iM as Toe =A ny che way, | beoanie really fond of the TV there. dun\n",
            "\n",
            ". ees fame. was thin und Nat like w board, but the one there ~\n",
            "\n",
            "te “3 yee “aemare of a box, very enticing for a cat. Plus, iC was fanny\n",
            "\n",
            "eltwarm, aid made my tummy ‘Feel toasty. Fantastic inthe\n",
            "\n",
            "was ‘ heinter, | imagined. ”: hg, vitiny iy, eet me\n",
            "\n",
            "ee +Ut's really old, Momo told n me. “In the Pest, all TViieen\n",
            "\n",
            "ee ? ahis chupe. ; apparently. ‘Going from this perfect design to an...\n",
            "\n",
            "o a Hf  Vepeactical flat shupe is. if ae att me, 3 ap hacks.\n",
            "\n",
            "ofits ta wo sR\n",
            "a!\n",
            "\n",
            "    \n",
            "   \n",
            " \n",
            "\n",
            "or Mume told me that. ya ‘coukd tell haw ‘old 3 cat us:\n",
            ": tee Pat “hy nbether 6 or not they Knew about these bay TVs. In hes:\n",
            "\n",
            "OE Faas, Chitaho gare privity ty making things camforitie |\n",
            "ao’ fur cats amd she dismissed the idea of petting one oft Pa:\n",
            "\n",
            "ee ze eye\n",
            "Wag eres IVA splendid decisiun, in my ‘opin, va\n",
            "wrt ters .\n",
            "Pn “Why the glum fouk? M youte bated oft ben Mike\n",
            "1 i buch, Memo id to me, Fy tis ar) .\n",
            "\n",
            ". we Sots She was stretching out her long lirabs ana pearty: i |\n",
            "\n",
            " \n",
            "\n",
            "7\n",
            "sie teh mag On Ree ‘She'd allowed foe the guest, t to ake shape seatones\n",
            "Sieg ges pabeT\\,. Baise let, ae a ae |\n",
            "if ” . ; a yy . . ma 3a ‘ a a . Ly > ; ne\n",
            "dog we a Oe af 7 ) . . . ‘ 1 - _#\n",
            "- (ee Sk ee * a . fre\n",
            "sete yey MAD pe\n",
            "‘ i‘) tay F es 2, * “5 7 5 i - bar\n",
            "- - ie ~ - Biuy\n",
            "\n",
            " \n",
            "\n",
            "ae AM OM\n",
            "\n",
            "we\n",
            "\n",
            "1 dont get ‘it., s\n",
            "Exactly, But. d\n",
            "te alike: ‘Satoru, |\n",
            "| Dear mevit $0\n",
            "Chikako, used\n",
            "ef, ‘Momo, clar\n",
            "This was going\n",
            "when these human\n",
            "* the: cat wha lived: v\n",
            "/ What did Sate\n",
            "“yf. a woman wh\n",
            "ha cat uns Sears’\n",
            "if what Twas think\n",
            "M ll, thar’ $ ne\n",
            "comes: to Chik\n",
            "\n",
            "\" conscience regandi\n",
            "\n",
            "° Sounds” the\n",
            "* Chikato ercied up\n",
            "\n",
            "fs SiS ae\n",
            "\n",
            "📌 Detected Language: unknown\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [21/Mar/2025 15:31:19] \"\u001b[31m\u001b[1mPOST /process_frame HTTP/1.1\u001b[0m\" 400 -\n",
            "INFO:werkzeug:127.0.0.1 - - [21/Mar/2025 15:31:20] \"POST /process_frame HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "🔊 Text-to-Speech conversion done!\n",
            "\n",
            "📌 Detected Language: tl\n",
            "\n",
            "📝 Extracted Text:\n",
            "a\n",
            "\n",
            "🌍 Translated Text:\n",
            "a\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [21/Mar/2025 15:31:32] \"POST /process_frame HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "🔊 Text-to-Speech conversion done!\n",
            "\n",
            "📌 Detected Language: unknown\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [21/Mar/2025 15:31:53] \"\u001b[31m\u001b[1mPOST /process_frame HTTP/1.1\u001b[0m\" 400 -\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "📌 Detected Language: unknown\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [21/Mar/2025 15:32:04] \"\u001b[31m\u001b[1mPOST /process_frame HTTP/1.1\u001b[0m\" 400 -\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "📌 Detected Language: unknown\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [21/Mar/2025 15:32:14] \"\u001b[31m\u001b[1mPOST /process_frame HTTP/1.1\u001b[0m\" 400 -\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "📌 Detected Language: en\n",
            "\n",
            "📝 Extracted Text:\n",
            "MMe Bok 8 be FM g ee ee\n",
            "for cats noe, . e, tthe, Mh\n",
            "\n",
            "‘Vd awier ‘cheaduntet gal someone like’s ion ‘ ae\n",
            "wn “loony and nnwnly when he drank: W\n",
            "age faa hath. he suddenly started pou\n",
            "a ee +“ pike he was cowering before him, :\n",
            "\n",
            "- » drinking isnt fun, then why\n",
            "toe . tee rop af the TV in the sitting room\n",
            "= : = he ‘talked. until Satoru finally fem\n",
            "fom the table.\n",
            "'S os)\" By the way, I became really fond of the’ WV,\n",
            "ie i @ home vas thin and Mat like as bourd, but ah\n",
            "“wes more ‘of abox, very enticing for\n",
            "“warm, and made my tummy fect,\n",
            "4 winter, imagined. © — .\n",
            "“NS really old, None told m me. In the, past. ail TN, “\n",
            "* ahs shape, apparenily, Going from this perfe oe\n",
            "“ ipyvactical flat ape’ is, if you ask, me,\n",
            "iene “Wise. -\n",
            "\n",
            ". Momo told me that you could well how old ae Cat Wis a)\n",
            "\n",
            "bp whether or not they knew about these boxy TVs. In thir”\n",
            "\n",
            "| house, Chikaho gave priority to making things comfortable: *\n",
            "hacas: znd she dismissed. the idea of g Bete 8 one of the ofa\n",
            "rs A splendid decision, i in my\" opinion. “23\n",
            "\n",
            "- ms\n",
            "\n",
            "thee cr = Rix\n",
            "\n",
            "eR, as Sa “tory Han\n",
            "\n",
            "do it? ' Ww. oo ay\n",
            "ashy \"agin ae\n",
            "\n",
            "™ Cycing the, ty hea\n",
            "\n",
            "ved the botte ole 7\n",
            "Mt,\n",
            "\n",
            "here Oy\n",
            "\n",
            "Sut the ‘ane |\n",
            "I Cat. Plus it was tiny:\n",
            "toasty Fantastic j in te\n",
            "\n",
            "ct design twan\n",
            "\n",
            "e\n",
            "\n",
            "mo Whar the aay lank: EE worry! be Pee ae | ide ae mo q° \"I ye cs\n",
            "\n",
            "A sen Bechoards ‘| a tay\n",
            "th ee\n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            "“AVhat “did Sator\n",
            "te\n",
            "Mf a ‘woman uh\n",
            "gf ut cat ius Satorus\n",
            "\n",
            "eye vols what. Lwas. thin\n",
            "\n",
            "2 Well, that sm\n",
            "a ta: ‘Chil\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [21/Mar/2025 15:33:02] \"POST /process_frame HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "🔊 Text-to-Speech conversion done!\n",
            "\n",
            "📌 Detected Language: unknown\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [21/Mar/2025 15:33:16] \"\u001b[31m\u001b[1mPOST /process_frame HTTP/1.1\u001b[0m\" 400 -\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "📌 Detected Language: unknown\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [21/Mar/2025 15:33:43] \"\u001b[31m\u001b[1mPOST /process_frame HTTP/1.1\u001b[0m\" 400 -\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "📌 Detected Language: unknown\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [21/Mar/2025 15:34:07] \"\u001b[31m\u001b[1mPOST /process_frame HTTP/1.1\u001b[0m\" 400 -\n"
          ]
        }
      ],
      "source": [
        "# # from flask import Flask, request, jsonify, send_file\n",
        "# # from flask_cors import CORS\n",
        "# # import os\n",
        "# # import pytesseract\n",
        "# # from langdetect import detect\n",
        "# # from gtts import gTTS\n",
        "# # from googletrans import Translator\n",
        "# # from PIL import Image\n",
        "# # from io import BytesIO\n",
        "# # import threading\n",
        "# # import time\n",
        "# # import pygame\n",
        "# # from pyngrok import ngrok\n",
        "# # import base64\n",
        "\n",
        "# # # Initialize Flask app\n",
        "# # app = Flask(__name__)\n",
        "# # CORS(app)  # Enable ngrok for public URL\n",
        "\n",
        "\n",
        "# # pytesseract.pytesseract.tesseract_cmd = r\"/usr/bin/tesseract\"\n",
        "\n",
        "# # # Global variables for TTS processing\n",
        "# # tts_lock = threading.Lock()\n",
        "# # last_tts_time = 0\n",
        "# # TTS_INTERVAL = 25  # Seconds\n",
        "\n",
        "# # ngrok_tunnel = ngrok.connect(5000)\n",
        "# # print(f\"Public URL: {ngrok_tunnel.public_url}\")\n",
        "\n",
        "# # # Function to process the image and generate response\n",
        "# # def process_image(image_path):\n",
        "# #     try:\n",
        "# #         # Open the image\n",
        "# #         img = Image.open(image_path)\n",
        "\n",
        "# #         # Extract text using Tesseract for language detection\n",
        "# #         raw_text_sample = pytesseract.image_to_string(img, lang=\"hin+tam+tel+eng\").strip()\n",
        "# #         detected_lang = detect(raw_text_sample) if raw_text_sample else \"unknown\"\n",
        "\n",
        "# #         print(f\"\\n📌 Detected Language: {detected_lang}\")\n",
        "\n",
        "# #         # Extract text\n",
        "# #         extracted_text = pytesseract.image_to_string(img, lang=\"hin+tam+tel+eng\").strip()\n",
        "# #         if not extracted_text:\n",
        "# #             return None, None, None\n",
        "\n",
        "# #         print(f\"\\n📝 Extracted Text:\\n{extracted_text}\")\n",
        "\n",
        "# #         # Translate non-English text to English\n",
        "# #         translator = Translator()\n",
        "# #         translated_text = extracted_text  # Default to extracted text\n",
        "\n",
        "# #         if detected_lang != \"en\":\n",
        "# #             try:\n",
        "# #                 translation_result = translator.translate(extracted_text, src=detected_lang, dest=\"en\")\n",
        "# #                 translated_text = translation_result.text if translation_result else \"Translation failed\"\n",
        "# #                 print(f\"\\n🌍 Translated Text:\\n{translated_text}\")\n",
        "# #             except Exception as e:\n",
        "# #                 print(f\"❌ Translation Error: {e}\")\n",
        "# #                 translated_text = \"Translation failed\"\n",
        "\n",
        "# #         # Save processed image (optional: could draw bounding boxes, etc.)\n",
        "# #         processed_image_path = \"processed_image.jpg\"\n",
        "# #         img.save(processed_image_path)\n",
        "\n",
        "# #         return extracted_text, translated_text, processed_image_path\n",
        "\n",
        "# #     except Exception as e:\n",
        "# #         print(f\"⚠️ Error: {e}\")\n",
        "# #         return None, None, None\n",
        "\n",
        "# # # Function to perform TTS processing\n",
        "# # def perform_tts(text):\n",
        "# #     global last_tts_time\n",
        "\n",
        "# #     with tts_lock:\n",
        "# #         current_time = time.time()\n",
        "# #         if current_time - last_tts_time >= TTS_INTERVAL:\n",
        "# #             last_tts_time = current_time\n",
        "\n",
        "# #             try:\n",
        "# #                 # Convert to speech\n",
        "# #                 output_audio = \"output.mp3\"\n",
        "\n",
        "# #                 # Ensure the previous file is closed before overwriting\n",
        "# #                 pygame.mixer.init()\n",
        "# #                 pygame.mixer.music.stop()\n",
        "# #                 pygame.mixer.quit()  # Fully release the audio file\n",
        "\n",
        "# #                 # Remove old file if it exists\n",
        "# #                 if os.path.exists(output_audio):\n",
        "# #                     os.remove(output_audio)\n",
        "\n",
        "# #                 # Generate speech\n",
        "# #                 tts = gTTS(text=text, lang=\"en\")\n",
        "# #                 tts.save(output_audio)\n",
        "\n",
        "# #                 print(\"\\n🔊 Text-to-Speech conversion done!\")\n",
        "# #                 pygame.mixer.init()\n",
        "# #                 pygame.mixer.music.load(output_audio)\n",
        "# #                 pygame.mixer.music.play()\n",
        "# #             except Exception as e:\n",
        "# #                 print(f\"⚠️ TTS Error: {e}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# # @app.route(\"/process_frame\", methods=[\"POST\"])\n",
        "# # def process_frame():\n",
        "# #     try:\n",
        "# #         # Check if an image is sent\n",
        "# #         if \"image\" not in request.files:\n",
        "# #             return jsonify({\"error\": \"No image uploaded.\"}), 400\n",
        "\n",
        "# #         # Save the received image\n",
        "# #         image_file = request.files[\"image\"]\n",
        "# #         image_path = \"received_image.jpg\"\n",
        "# #         image_file.save(image_path)\n",
        "\n",
        "# #         # Process the image\n",
        "# #         extracted_text, translated_text, processed_image_path = process_image(image_path)\n",
        "\n",
        "# #         if not extracted_text:\n",
        "# #             return jsonify({\"error\": \"No text detected in the image.\"}), 400\n",
        "\n",
        "# # # Generate TTS audio\n",
        "# # output_audio = f\"temp_audio_{time.time()}.mp3\"  # Use a unique filename\n",
        "# # try:\n",
        "# #     # Generate TTS audio\n",
        "# #     tts = gTTS(text=translated_text, lang=\"en\")\n",
        "# #     tts.save(output_audio)\n",
        "# #     print(\"\\n🔊 Text-to-Speech conversion done!\")\n",
        "\n",
        "# #     # Read the file for Base64 encoding\n",
        "# #     with open(output_audio, \"rb\") as audio_file:\n",
        "# #         audio_base64 = base64.b64encode(audio_file.read()).decode(\"utf-8\")\n",
        "\n",
        "# #     # Clean up the temporary audio file\n",
        "# #     os.remove(output_audio)\n",
        "\n",
        "# # except Exception as e:\n",
        "# #     print(f\"⚠️ Audio Playback Error: {e}\")\n",
        "# #     audio_base64 = None\n",
        "\n",
        "\n",
        "# #         # Encode processed image as Base64\n",
        "# #         with open(processed_image_path, \"rb\") as img_file:\n",
        "# #             image_base64 = base64.b64encode(img_file.read()).decode(\"utf-8\")\n",
        "\n",
        "# #         # Encode audio file as Base64 (if it exists)\n",
        "# #         audio_base64 = None\n",
        "# #         if output_audio and os.path.exists(output_audio):\n",
        "# #             with open(output_audio, \"rb\") as audio_file:\n",
        "# #                 audio_base64 = base64.b64encode(audio_file.read()).decode(\"utf-8\")\n",
        "\n",
        "# #         # Create JSON response\n",
        "# #         response = {\n",
        "# #             \"extracted_text\": extracted_text,\n",
        "# #             \"translated_text\": translated_text,\n",
        "# #             \"processed_image\": image_base64,\n",
        "# #             \"audio\": audio_base64,\n",
        "# #         }\n",
        "\n",
        "# #         return jsonify(response), 200\n",
        "\n",
        "# #     except Exception as e:\n",
        "# #         print(f\"⚠️ Server Error: {e}\")\n",
        "# #         return jsonify({\"error\": str(e)}), 500\n",
        "\n",
        "\n",
        "# # # Main function\n",
        "# # if __name__ == \"__main__\":\n",
        "# #     app.run(port=5000)\n",
        "# # # from flask import Flask, request, jsonify\n",
        "# # # from flask_cors import CORS\n",
        "# # # import os\n",
        "# # # import pytesseract\n",
        "# # # from langdetect import detect\n",
        "# # # from gtts import gTTS\n",
        "# # # from googletrans import Translator\n",
        "# # # from PIL import Image\n",
        "# # # from io import BytesIO\n",
        "# # # import threading\n",
        "# # # import time\n",
        "# # # import base64\n",
        "# # # import cv2\n",
        "# # # import numpy as np\n",
        "\n",
        "# # # # Initialize Flask app\n",
        "# # # app = Flask(__name__)\n",
        "# # # CORS(app)  # Enable ngrok for public URL\n",
        "\n",
        "# # # pytesseract.pytesseract.tesseract_cmd = r\"/usr/bin/tesseract\"\n",
        "\n",
        "# # # ngrok_tunnel = ngrok.connect(5000)\n",
        "# # # print(f\"Public URL: {ngrok_tunnel.public_url}\")\n",
        "\n",
        "# # # # Frame preprocessing function\n",
        "# # # def preprocess_frame(frame):\n",
        "# # #     # Convert to grayscale\n",
        "# # #     gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "# # #     # Upscale the image\n",
        "# # #     upscale_factor = 4\n",
        "# # #     upscaled = cv2.resize(gray, (gray.shape[1] * upscale_factor, gray.shape[0] * upscale_factor))\n",
        "\n",
        "# # #     # Remove salt-and-pepper noise\n",
        "# # #     denoised = cv2.medianBlur(upscaled, 1)\n",
        "\n",
        "# # #     # Apply Gaussian blur\n",
        "# # #     blurred = cv2.GaussianBlur(denoised, (9, 9), 0)\n",
        "\n",
        "# # #     # Apply adaptive thresholding\n",
        "# # #     thresh = cv2.adaptiveThreshold(blurred, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 35, 2)\n",
        "\n",
        "# # #     # Dilation with a modified kernel size\n",
        "# # #     kernel = np.ones((1, 1), np.uint8)\n",
        "# # #     dilated = cv2.dilate(thresh, kernel, iterations=1)\n",
        "\n",
        "# # #     return dilated\n",
        "\n",
        "# # # # Function to process the image and generate response\n",
        "# # # def process_image(image_path):\n",
        "# # #     try:\n",
        "# # #         # Open the image\n",
        "# # #         img = Image.open(image_path)\n",
        "\n",
        "# # #         # Extract text using Tesseract for language detection\n",
        "# # #         raw_text_sample = pytesseract.image_to_string(img, lang=\"hin+tam+tel+eng\").strip()\n",
        "# # #         detected_lang = detect(raw_text_sample) if raw_text_sample else \"unknown\"\n",
        "\n",
        "# # #         print(f\"\\n📌 Detected Language: {detected_lang}\")\n",
        "\n",
        "# # #         # Extract text\n",
        "# # #         extracted_text = pytesseract.image_to_string(img, lang=\"hin+tam+tel+eng\").strip()\n",
        "# # #         if not extracted_text:\n",
        "# # #             return None, None, None\n",
        "\n",
        "# # #         print(f\"\\n📝 Extracted Text:\\n{extracted_text}\")\n",
        "\n",
        "# # #         # Translate non-English text to English\n",
        "# # #         translator = Translator()\n",
        "# # #         translated_text = extracted_text  # Default to extracted text\n",
        "\n",
        "# # #         if detected_lang != \"en\":\n",
        "# # #             try:\n",
        "# # #                 translation_result = translator.translate(extracted_text, src=detected_lang, dest=\"en\")\n",
        "# # #                 translated_text = translation_result.text if translation_result else \"Translation failed\"\n",
        "# # #                 print(f\"\\n🌍 Translated Text:\\n{translated_text}\")\n",
        "# # #             except Exception as e:\n",
        "# # #                 print(f\"❌ Translation Error: {e}\")\n",
        "# # #                 translated_text = \"Translation failed\"\n",
        "\n",
        "# # #         # Save processed image (optional: could draw bounding boxes, etc.)\n",
        "# # #         processed_image_path = \"processed_image.jpg\"\n",
        "# # #         img.save(processed_image_path)\n",
        "\n",
        "# # #         return extracted_text, translated_text, processed_image_path\n",
        "\n",
        "# # #     except Exception as e:\n",
        "# # #         print(f\"⚠️ Error: {e}\")\n",
        "# # #         return None, None, None\n",
        "\n",
        "# # # @app.route(\"/process_frame\", methods=[\"POST\"])\n",
        "# # # def process_frame():\n",
        "# # #     try:\n",
        "# # #         # Check if an image is sent\n",
        "# # #         if \"image\" not in request.files:\n",
        "# # #             return jsonify({\"error\": \"No image uploaded.\"}), 400\n",
        "\n",
        "# # #         # Save the received image\n",
        "# # #         image_file = request.files[\"image\"]\n",
        "# # #         image_path = \"received_image.jpg\"\n",
        "# # #         image_file.save(image_path)\n",
        "\n",
        "# # #         # Read the image using OpenCV\n",
        "# # #         frame = cv2.imread(image_path)\n",
        "# # #         if frame is None:\n",
        "# # #             return jsonify({\"error\": \"Failed to read the image.\"}), 400\n",
        "\n",
        "# # #         # Preprocess the frame\n",
        "# # #         preprocessed_frame = preprocess_frame(frame)\n",
        "\n",
        "# # #         # Save the preprocessed frame\n",
        "# # #         preprocessed_image_path = \"preprocessed_image.jpg\"\n",
        "# # #         cv2.imwrite(preprocessed_image_path, preprocessed_frame)\n",
        "\n",
        "# # #         # Process the preprocessed frame\n",
        "# # #         extracted_text, translated_text, processed_image_path = process_image(preprocessed_image_path)\n",
        "\n",
        "# # #         if not extracted_text:\n",
        "# # #             return jsonify({\"error\": \"No text detected in the image.\"}), 400\n",
        "\n",
        "# # #         # Generate TTS audio synchronously\n",
        "# # #         output_audio = \"output.mp3\"\n",
        "# # #         try:\n",
        "# # #             # Remove old audio file if it exists\n",
        "# # #             if os.path.exists(output_audio):\n",
        "# # #                 os.remove(output_audio)\n",
        "\n",
        "# # #             # Generate TTS audio\n",
        "# # #             tts = gTTS(text=translated_text, lang=\"en\")\n",
        "# # #             tts.save(output_audio)\n",
        "# # #             print(\"\\n🔊 Text-to-Speech conversion done!\")\n",
        "# # #         except Exception as e:\n",
        "# # #             print(f\"⚠️ TTS Error: {e}\")\n",
        "# # #             output_audio = None\n",
        "\n",
        "# # #         # Encode processed image as Base64\n",
        "# # #         with open(processed_image_path, \"rb\") as img_file:\n",
        "# # #             image_base64 = base64.b64encode(img_file.read()).decode(\"utf-8\")\n",
        "\n",
        "# # #         # Encode audio file as Base64 (if it exists)\n",
        "# # #         audio_base64 = None\n",
        "# # #         if output_audio and os.path.exists(output_audio):\n",
        "# # #             with open(output_audio, \"rb\") as audio_file:\n",
        "# # #                 audio_base64 = base64.b64encode(audio_file.read()).decode(\"utf-8\")\n",
        "\n",
        "# # #         # Create JSON response\n",
        "# # #         response = {\n",
        "# # #             \"extracted_text\": extracted_text,\n",
        "# # #             \"translated_text\": translated_text,\n",
        "# # #             \"processed_image\": image_base64,\n",
        "# # #             \"audio\": audio_base64,\n",
        "# # #         }\n",
        "\n",
        "# # #         return jsonify(response), 200\n",
        "\n",
        "# # #     except Exception as e:\n",
        "# # #         print(f\"⚠️ Server Error: {e}\")\n",
        "# # #         return jsonify({\"error\": str(e)}), 500\n",
        "\n",
        "# # # def periodic_processing():\n",
        "# # #     while True:\n",
        "# # #         time.sleep(7)\n",
        "# # #         # Add any background processing logic here\n",
        "# # #         print(\"Periodic processing executed.\")\n",
        "\n",
        "# # # if __name__ == \"__main__\":\n",
        "# # #     threading.Thread(target=periodic_processing, daemon=True).start()\n",
        "\n",
        "# # #     app.run(port=5000)\n",
        "\n",
        "# from flask import Flask, request, jsonify\n",
        "# from flask_cors import CORS\n",
        "# import os\n",
        "# import pytesseract\n",
        "# from langdetect import detect\n",
        "# from gtts import gTTS\n",
        "# from googletrans import Translator\n",
        "# from PIL import Image\n",
        "# from io import BytesIO\n",
        "# import threading\n",
        "# import time\n",
        "# import base64\n",
        "# import cv2\n",
        "# import numpy as np\n",
        "\n",
        "# # Initialize Flask app\n",
        "# app = Flask(__name__)\n",
        "# CORS(app)\n",
        "\n",
        "# pytesseract.pytesseract.tesseract_cmd = r\"/usr/bin/tesseract\"\n",
        "\n",
        "# ngrok_tunnel = ngrok.connect(5000)\n",
        "# print(f\"Public URL: {ngrok_tunnel.public_url}\")\n",
        "\n",
        "# # Frame preprocessing function\n",
        "# def preprocess_frame(frame):\n",
        "#     # Convert to grayscale\n",
        "#     gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "#     # Upscale the image\n",
        "#     upscale_factor = 4\n",
        "#     upscaled = cv2.resize(gray, (gray.shape[1] * upscale_factor, gray.shape[0] * upscale_factor))\n",
        "\n",
        "#     # Remove salt-and-pepper noise\n",
        "#     denoised = cv2.medianBlur(upscaled, 5)\n",
        "\n",
        "#     # Apply Gaussian blur\n",
        "#     blurred = cv2.GaussianBlur(denoised, (9, 9), 0)\n",
        "\n",
        "#     # Apply adaptive thresholding\n",
        "#     thresh = cv2.adaptiveThreshold(blurred, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 35, 2)\n",
        "\n",
        "#     # Dilation with a modified kernel size\n",
        "#     kernel = np.ones((1, 1), np.uint8)\n",
        "#     dilated = cv2.dilate(thresh, kernel, iterations=1)\n",
        "\n",
        "#     return dilated\n",
        "\n",
        "# # Function to process the image and generate response\n",
        "# def process_image(image_path):\n",
        "#     try:\n",
        "#         # Open the image\n",
        "#         img = Image.open(image_path)\n",
        "\n",
        "#         # Extract text using Tesseract for language detection\n",
        "#         raw_text_sample = pytesseract.image_to_string(img, lang=\"hin+tam+tel+eng\").strip()\n",
        "#         detected_lang = detect(raw_text_sample) if raw_text_sample else \"unknown\"\n",
        "\n",
        "#         print(f\"\\n📌 Detected Language: {detected_lang}\")\n",
        "\n",
        "#         # Extract text\n",
        "#         extracted_text = pytesseract.image_to_string(img, lang=\"hin+tam+tel+eng\").strip()\n",
        "#         if not extracted_text:\n",
        "#             return None, None, None\n",
        "\n",
        "#         print(f\"\\n📝 Extracted Text:\\n{extracted_text}\")\n",
        "\n",
        "#         # Translate non-English text to English\n",
        "#         translator = Translator()\n",
        "#         translated_text = extracted_text  # Default to extracted text\n",
        "\n",
        "#         if detected_lang != \"en\":\n",
        "#             try:\n",
        "#                 translation_result = translator.translate(extracted_text, src=detected_lang, dest=\"en\")\n",
        "#                 translated_text = translation_result.text if translation_result else \"Translation failed\"\n",
        "#                 print(f\"\\n🌍 Translated Text:\\n{translated_text}\")\n",
        "#             except Exception as e:\n",
        "#                 print(f\"❌ Translation Error: {e}\")\n",
        "#                 translated_text = \"Translation failed\"\n",
        "\n",
        "#         # Save processed image (optional: could draw bounding boxes, etc.)\n",
        "#         processed_image_path = \"processed_image.jpg\"\n",
        "#         img.save(processed_image_path)\n",
        "\n",
        "#         return extracted_text, translated_text, processed_image_path\n",
        "\n",
        "#     except Exception as e:\n",
        "#         print(f\"⚠️ Error: {e}\")\n",
        "#         return None, None, None\n",
        "\n",
        "# @app.route(\"/process_frame\", methods=[\"POST\"])\n",
        "# def process_frame():\n",
        "#     try:\n",
        "#         # Check if an image is sent\n",
        "#         if \"image\" not in request.files:\n",
        "#             return jsonify({\"error\": \"No image uploaded.\"}), 400\n",
        "\n",
        "#         # Save the received image\n",
        "#         image_file = request.files[\"image\"]\n",
        "#         image_path = \"received_image.jpg\"\n",
        "#         image_file.save(image_path)\n",
        "\n",
        "#         # Read the image using OpenCV\n",
        "#         frame = cv2.imread(image_path)\n",
        "#         if frame is None:\n",
        "#             return jsonify({\"error\": \"Failed to read the image.\"}), 400\n",
        "\n",
        "#         # Preprocess the frame\n",
        "#         preprocessed_frame = preprocess_frame(frame)\n",
        "\n",
        "#         # Save the preprocessed frame\n",
        "#         preprocessed_image_path = \"preprocessed_image.jpg\"\n",
        "#         cv2.imwrite(preprocessed_image_path, preprocessed_frame)\n",
        "\n",
        "#         # Process the preprocessed frame\n",
        "#         extracted_text, translated_text, processed_image_path = process_image(preprocessed_image_path)\n",
        "\n",
        "#         if not extracted_text:\n",
        "#             return jsonify({\"error\": \"No text detected in the image.\"}), 400\n",
        "\n",
        "#        # Generate TTS audio synchronously\n",
        "#         output_audio = f\"temp_audio_{time.time()}.mp3\"  # Use a unique filename\n",
        "#         try:\n",
        "#             # Generate TTS audio\n",
        "#             tts = gTTS(text=translated_text, lang=\"en\")\n",
        "#             tts.save(output_audio)\n",
        "#             print(\"\\n🔊 Text-to-Speech conversion done!\")\n",
        "\n",
        "#             # Read the file for Base64 encoding\n",
        "#             with open(output_audio, \"rb\") as audio_file:\n",
        "#                 audio_base64 = base64.b64encode(audio_file.read()).decode(\"utf-8\")\n",
        "\n",
        "#             # Clean up the temporary audio file\n",
        "#             os.remove(output_audio)\n",
        "\n",
        "#         except Exception as e:\n",
        "#             print(f\"⚠️ Audio Playback Error: {e}\")\n",
        "#             audio_base64 = None\n",
        "\n",
        "#         # Encode processed image as Base64\n",
        "#         with open(processed_image_path, \"rb\") as img_file:\n",
        "#             image_base64 = base64.b64encode(img_file.read()).decode(\"utf-8\")\n",
        "\n",
        "#         # Create JSON response\n",
        "#         response = {\n",
        "#             \"extracted_text\": extracted_text,\n",
        "#             \"translated_text\": translated_text,\n",
        "#             \"processed_image\": image_base64,\n",
        "#             \"audio\": audio_base64,\n",
        "#         }\n",
        "\n",
        "#         return jsonify(response), 200\n",
        "\n",
        "#     except Exception as e:\n",
        "#         print(f\"⚠️ Server Error: {e}\")\n",
        "#         return jsonify({\"error\": str(e)}), 500\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "#     app.run(port=5000)\n",
        "\n",
        "\n",
        "from flask import Flask, request, jsonify\n",
        "from flask_cors import CORS\n",
        "import os\n",
        "import pytesseract\n",
        "from langdetect import detect\n",
        "from gtts import gTTS\n",
        "from googletrans import Translator\n",
        "from PIL import Image\n",
        "from io import BytesIO\n",
        "import threading\n",
        "import time\n",
        "import base64\n",
        "import cv2\n",
        "import numpy as np\n",
        "from pyngrok import ngrok\n",
        "\n",
        "\n",
        "# Initialize Flask app\n",
        "app = Flask(__name__)\n",
        "CORS(app)\n",
        "\n",
        "pytesseract.pytesseract.tesseract_cmd = r\"/usr/bin/tesseract\"\n",
        "\n",
        "ngrok_tunnel = ngrok.connect(5000)\n",
        "print(f\"Public URL: {ngrok_tunnel.public_url}\")\n",
        "\n",
        "# Frame preprocessing function\n",
        "def preprocess_frame(frame):\n",
        "    # Convert to grayscale\n",
        "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    # Upscale the image\n",
        "    upscale_factor = 4\n",
        "    upscaled = cv2.resize(gray, (gray.shape[1] * upscale_factor, gray.shape[0] * upscale_factor))\n",
        "\n",
        "    # Remove salt-and-pepper noise\n",
        "    denoised = cv2.medianBlur(upscaled, 5)\n",
        "\n",
        "    # Apply Gaussian blur\n",
        "    blurred = cv2.GaussianBlur(denoised, (9, 9), 0)\n",
        "\n",
        "    # Apply adaptive thresholding\n",
        "    thresh = cv2.adaptiveThreshold(blurred, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 35, 2)\n",
        "\n",
        "    # Dilation with a modified kernel size\n",
        "    kernel = np.ones((1, 1), np.uint8)\n",
        "    dilated = cv2.dilate(thresh, kernel, iterations=1)\n",
        "\n",
        "    return dilated\n",
        "\n",
        "# Function to process the image and generate response\n",
        "def process_image(image_path):\n",
        "    try:\n",
        "        # Open the image\n",
        "        img = Image.open(image_path)\n",
        "\n",
        "        # Extract text using Tesseract for language detection\n",
        "        raw_text_sample = pytesseract.image_to_string(img, lang=\"hin+tam+tel+eng\").strip()\n",
        "        detected_lang = detect(raw_text_sample) if raw_text_sample else \"unknown\"\n",
        "\n",
        "        print(f\"\\n📌 Detected Language: {detected_lang}\")\n",
        "\n",
        "        # Extract text\n",
        "        extracted_text = pytesseract.image_to_string(img, lang=\"hin+tam+tel+eng\").strip()\n",
        "        if not extracted_text:\n",
        "            return None, None, None\n",
        "\n",
        "        print(f\"\\n📝 Extracted Text:\\n{extracted_text}\")\n",
        "\n",
        "        # Translate non-English text to English\n",
        "        translator = Translator()\n",
        "        translated_text = extracted_text  # Default to extracted text\n",
        "\n",
        "        if detected_lang != \"en\":\n",
        "            try:\n",
        "                translation_result = translator.translate(extracted_text, src=detected_lang, dest=\"en\")\n",
        "                translated_text = translation_result.text if translation_result else \"Translation failed\"\n",
        "                print(f\"\\n🌍 Translated Text:\\n{translated_text}\")\n",
        "            except Exception as e:\n",
        "                print(f\"❌ Translation Error: {e}\")\n",
        "                translated_text = \"Translation failed\"\n",
        "\n",
        "        # Save processed image (optional: could draw bounding boxes, etc.)\n",
        "        processed_image_path = \"processed_image.jpg\"\n",
        "        img.save(processed_image_path)\n",
        "\n",
        "        return extracted_text, translated_text, processed_image_path\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️ Error: {e}\")\n",
        "        return None, None, None\n",
        "\n",
        "@app.route(\"/process_frame\", methods=[\"POST\"])\n",
        "def process_frame():\n",
        "    try:\n",
        "        # Check if an image is sent\n",
        "        if \"image\" not in request.files:\n",
        "            return jsonify({\"error\": \"No image uploaded.\"}), 400\n",
        "\n",
        "        # Save the received image\n",
        "        image_file = request.files[\"image\"]\n",
        "        image_path = \"received_image.jpg\"\n",
        "        image_file.save(image_path)\n",
        "\n",
        "        # Read the image using OpenCV\n",
        "        frame = cv2.imread(image_path)\n",
        "        if frame is None:\n",
        "            return jsonify({\"error\": \"Failed to read the image.\"}), 400\n",
        "\n",
        "        # Preprocess the frame\n",
        "        preprocessed_frame = preprocess_frame(frame)\n",
        "\n",
        "        # Save the preprocessed frame\n",
        "        preprocessed_image_path = \"preprocessed_image.jpg\"\n",
        "        cv2.imwrite(preprocessed_image_path, preprocessed_frame)\n",
        "\n",
        "        # Process the preprocessed frame\n",
        "        extracted_text, translated_text, processed_image_path = process_image(preprocessed_image_path)\n",
        "\n",
        "        if not extracted_text:\n",
        "            return jsonify({\"error\": \"No text detected in the image.\"}), 400\n",
        "\n",
        "        # Generate TTS audio synchronously\n",
        "        output_audio = f\"temp_audio_{time.time()}.mp3\"  # Use a unique filename\n",
        "        try:\n",
        "            # Generate TTS audio\n",
        "            tts = gTTS(text=translated_text, lang=\"en\")\n",
        "            tts.save(output_audio)\n",
        "            print(\"\\n🔊 Text-to-Speech conversion done!\")\n",
        "\n",
        "            # Read the file for Base64 encoding\n",
        "            with open(output_audio, \"rb\") as audio_file:\n",
        "                audio_base64 = base64.b64encode(audio_file.read()).decode(\"utf-8\")\n",
        "\n",
        "            # Clean up the temporary audio file\n",
        "            os.remove(output_audio)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"⚠️ Audio Playback Error: {e}\")\n",
        "            audio_base64 = None\n",
        "\n",
        "        # Encode processed image as Base64\n",
        "        with open(processed_image_path, \"rb\") as img_file:\n",
        "            image_base64 = base64.b64encode(img_file.read()).decode(\"utf-8\")\n",
        "\n",
        "        # Create JSON response\n",
        "        response = {\n",
        "            \"extracted_text\": extracted_text,\n",
        "            \"translated_text\": translated_text,\n",
        "            \"processed_image\": image_base64,\n",
        "            \"audio\": audio_base64,\n",
        "        }\n",
        "\n",
        "        return jsonify(response), 200\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️ Server Error: {e}\")\n",
        "        return jsonify({\"error\": str(e)}), 500\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    app.run(port=5000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cvGJqCa9vyem",
        "outputId": "3a94be93-4f5e-4b0c-8e30-3461770a9648"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hit:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Get:2 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\n",
            "Get:3 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:5 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Get:8 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [8,763 kB]\n",
            "Get:9 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,237 kB]\n",
            "Hit:10 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:11 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:12 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:13 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,678 kB]\n",
            "Get:14 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [2,692 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [3,003 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,535 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [3,962 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu jammy-backports/universe amd64 Packages [38.5 kB]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu jammy-backports/main amd64 Packages [112 kB]\n",
            "Fetched 24.4 MB in 8s (3,214 kB/s)\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  tesseract-ocr-eng tesseract-ocr-osd\n",
            "The following NEW packages will be installed:\n",
            "  tesseract-ocr tesseract-ocr-eng tesseract-ocr-osd\n",
            "0 upgraded, 3 newly installed, 0 to remove and 30 not upgraded.\n",
            "Need to get 4,816 kB of archives.\n",
            "After this operation, 15.6 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-eng all 1:4.00~git30-7274cfa-1.1 [1,591 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-osd all 1:4.00~git30-7274cfa-1.1 [2,990 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr amd64 4.1.1-2.1build1 [236 kB]\n",
            "Fetched 4,816 kB in 1s (4,282 kB/s)\n",
            "Selecting previously unselected package tesseract-ocr-eng.\n",
            "(Reading database ... 126209 files and directories currently installed.)\n",
            "Preparing to unpack .../tesseract-ocr-eng_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-eng (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-osd.\n",
            "Preparing to unpack .../tesseract-ocr-osd_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-osd (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr.\n",
            "Preparing to unpack .../tesseract-ocr_4.1.1-2.1build1_amd64.deb ...\n",
            "Unpacking tesseract-ocr (4.1.1-2.1build1) ...\n",
            "Setting up tesseract-ocr-eng (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-osd (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr (4.1.1-2.1build1) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n"
          ]
        }
      ],
      "source": [
        "!apt-get update\n",
        "!apt-get install -y tesseract-ocr\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JVNVQgRaX23x"
      },
      "source": [
        "//////////////////////////////////////////combine/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IYu7aTFplj_N",
        "outputId": "d746d7b3-109c-4cfb-b768-d7c34cdc6da1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zua6vVj5lj5Y",
        "outputId": "538be226-1d12-46d9-c066-abffd23157c6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyngrok\n",
            "  Downloading pyngrok-7.2.4-py3-none-any.whl.metadata (8.7 kB)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.11/dist-packages (from pyngrok) (6.0.2)\n",
            "Downloading pyngrok-7.2.4-py3-none-any.whl (23 kB)\n",
            "Installing collected packages: pyngrok\n",
            "Successfully installed pyngrok-7.2.4\n",
            "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n"
          ]
        }
      ],
      "source": [
        "!pip install pyngrok\n",
        "!ngrok config add-authtoken 2t7eHroGCkLrnrgTlZkzdWEuVLP_2CWMekhkpnc894q1eXwLA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ia_UoQfKljyO",
        "outputId": "ed86267c-c2b9-4f14-93a9-49d90b771b88"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: flask in /usr/local/lib/python3.11/dist-packages (3.1.0)\n",
            "Collecting flask-cors\n",
            "  Downloading flask_cors-5.0.1-py3-none-any.whl.metadata (961 bytes)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (4.11.0.86)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Collecting gtts\n",
            "  Downloading gTTS-2.5.4-py3-none-any.whl.metadata (4.1 kB)\n",
            "Collecting ultralytics\n",
            "  Downloading ultralytics-8.3.108-py3-none-any.whl.metadata (37 kB)\n",
            "Collecting deepface\n",
            "  Downloading deepface-0.0.93-py3-none-any.whl.metadata (30 kB)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (11.1.0)\n",
            "Requirement already satisfied: pyngrok in /usr/local/lib/python3.11/dist-packages (7.2.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: Werkzeug>=3.1 in /usr/local/lib/python3.11/dist-packages (from flask) (3.1.3)\n",
            "Requirement already satisfied: Jinja2>=3.1.2 in /usr/local/lib/python3.11/dist-packages (from flask) (3.1.6)\n",
            "Requirement already satisfied: itsdangerous>=2.2 in /usr/local/lib/python3.11/dist-packages (from flask) (2.2.0)\n",
            "Requirement already satisfied: click>=8.1.3 in /usr/local/lib/python3.11/dist-packages (from flask) (8.1.8)\n",
            "Requirement already satisfied: blinker>=1.9 in /usr/local/lib/python3.11/dist-packages (from flask) (1.9.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.11/dist-packages (from gtts) (2.32.3)\n",
            "Requirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (3.10.0)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (6.0.2)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (1.14.1)\n",
            "Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (0.21.0+cu124)\n",
            "Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (4.67.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from ultralytics) (5.9.5)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.11/dist-packages (from ultralytics) (9.0.0)\n",
            "Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.2.2)\n",
            "Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (0.13.2)\n",
            "Collecting ultralytics-thop>=2.0.0 (from ultralytics)\n",
            "  Downloading ultralytics_thop-2.0.14-py3-none-any.whl.metadata (9.4 kB)\n",
            "Requirement already satisfied: gdown>=3.10.1 in /usr/local/lib/python3.11/dist-packages (from deepface) (5.2.0)\n",
            "Requirement already satisfied: tensorflow>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from deepface) (2.18.0)\n",
            "Requirement already satisfied: keras>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from deepface) (3.8.0)\n",
            "Collecting mtcnn>=0.1.0 (from deepface)\n",
            "  Downloading mtcnn-1.0.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "Collecting retina-face>=0.0.1 (from deepface)\n",
            "  Downloading retina_face-0.0.17-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting fire>=0.4.0 (from deepface)\n",
            "  Downloading fire-0.7.0.tar.gz (87 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.2/87.2 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting gunicorn>=20.1.0 (from deepface)\n",
            "  Downloading gunicorn-23.0.0-py3-none-any.whl.metadata (4.4 kB)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.11/dist-packages (from fire>=0.4.0->deepface) (3.0.1)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from gdown>=3.10.1->deepface) (4.13.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from gunicorn>=20.1.0->deepface) (24.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from Jinja2>=3.1.2->flask) (3.0.2)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from keras>=2.2.0->deepface) (1.4.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=2.2.0->deepface) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=2.2.0->deepface) (0.0.8)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.11/dist-packages (from keras>=2.2.0->deepface) (3.13.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=2.2.0->deepface) (0.15.0)\n",
            "Requirement already satisfied: ml-dtypes in /usr/local/lib/python3.11/dist-packages (from keras>=2.2.0->deepface) (0.4.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.8)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.8.2)\n",
            "Requirement already satisfied: joblib>=1.4.2 in /usr/local/lib/python3.11/dist-packages (from mtcnn>=0.1.0->deepface) (1.4.2)\n",
            "Collecting lz4>=4.3.3 (from mtcnn>=0.1.0->deepface)\n",
            "  Downloading lz4-4.4.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.4->ultralytics) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.4->ultralytics) (2025.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->gtts) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->gtts) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->gtts) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->gtts) (2025.1.31)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=1.9.0->deepface) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=1.9.0->deepface) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=1.9.0->deepface) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=1.9.0->deepface) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=1.9.0->deepface) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=1.9.0->deepface) (3.4.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=1.9.0->deepface) (5.29.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow>=1.9.0->deepface) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=1.9.0->deepface) (1.17.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=1.9.0->deepface) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=1.9.0->deepface) (1.71.0)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=1.9.0->deepface) (2.18.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=1.9.0->deepface) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow>=1.9.0->deepface) (0.45.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow>=1.9.0->deepface) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow>=1.9.0->deepface) (0.7.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->gdown>=3.10.1->deepface) (2.6)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown>=3.10.1->deepface) (1.7.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=2.2.0->deepface) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=2.2.0->deepface) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=2.2.0->deepface) (0.1.2)\n",
            "Downloading flask_cors-5.0.1-py3-none-any.whl (11 kB)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m47.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m29.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m37.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m76.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gTTS-2.5.4-py3-none-any.whl (29 kB)\n",
            "Downloading ultralytics-8.3.108-py3-none-any.whl (974 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m974.8/974.8 kB\u001b[0m \u001b[31m45.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading deepface-0.0.93-py3-none-any.whl (108 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m108.6/108.6 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gunicorn-23.0.0-py3-none-any.whl (85 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.0/85.0 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mtcnn-1.0.0-py3-none-any.whl (1.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m67.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading retina_face-0.0.17-py3-none-any.whl (25 kB)\n",
            "Downloading ultralytics_thop-2.0.14-py3-none-any.whl (26 kB)\n",
            "Downloading lz4-4.4.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m41.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: fire\n",
            "  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fire: filename=fire-0.7.0-py3-none-any.whl size=114249 sha256=eaa4c048e4135e854cd75a6916113b4dc2875f2c21c4c0b1673b2c75694f7e05\n",
            "  Stored in directory: /root/.cache/pip/wheels/46/54/24/1624fd5b8674eb1188623f7e8e17cdf7c0f6c24b609dfb8a89\n",
            "Successfully built fire\n",
            "Installing collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, lz4, gunicorn, fire, nvidia-cusparse-cu12, nvidia-cudnn-cu12, mtcnn, gtts, nvidia-cusolver-cu12, flask-cors, ultralytics-thop, retina-face, ultralytics, deepface\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed deepface-0.0.93 fire-0.7.0 flask-cors-5.0.1 gtts-2.5.4 gunicorn-23.0.0 lz4-4.4.4 mtcnn-1.0.0 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 retina-face-0.0.17 ultralytics-8.3.108 ultralytics-thop-2.0.14\n"
          ]
        }
      ],
      "source": [
        "!pip install flask flask-cors opencv-python torch gtts ultralytics deepface pillow pyngrok numpy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "skXo7PEEljjZ",
        "outputId": "5d062c5e-42fe-4d20-d3d8-64f199c91700"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting mediapipe\n",
            "  Downloading mediapipe-0.10.21-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from mediapipe) (1.4.0)\n",
            "Requirement already satisfied: attrs>=19.1.0 in /usr/local/lib/python3.11/dist-packages (from mediapipe) (25.3.0)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.11/dist-packages (from mediapipe) (25.2.10)\n",
            "Requirement already satisfied: jax in /usr/local/lib/python3.11/dist-packages (from mediapipe) (0.5.2)\n",
            "Requirement already satisfied: jaxlib in /usr/local/lib/python3.11/dist-packages (from mediapipe) (0.5.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from mediapipe) (3.10.0)\n",
            "Collecting numpy<2 (from mediapipe)\n",
            "  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: opencv-contrib-python in /usr/local/lib/python3.11/dist-packages (from mediapipe) (4.11.0.86)\n",
            "Collecting protobuf<5,>=4.25.3 (from mediapipe)\n",
            "  Downloading protobuf-4.25.6-cp37-abi3-manylinux2014_x86_64.whl.metadata (541 bytes)\n",
            "Collecting sounddevice>=0.4.4 (from mediapipe)\n",
            "  Downloading sounddevice-0.5.1-py3-none-any.whl.metadata (1.4 kB)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (from mediapipe) (0.2.0)\n",
            "Requirement already satisfied: CFFI>=1.0 in /usr/local/lib/python3.11/dist-packages (from sounddevice>=0.4.4->mediapipe) (1.17.1)\n",
            "Requirement already satisfied: ml_dtypes>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from jax->mediapipe) (0.4.1)\n",
            "Requirement already satisfied: opt_einsum in /usr/local/lib/python3.11/dist-packages (from jax->mediapipe) (3.4.0)\n",
            "Requirement already satisfied: scipy>=1.11.1 in /usr/local/lib/python3.11/dist-packages (from jax->mediapipe) (1.14.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (2.8.2)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe) (2.22)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib->mediapipe) (1.17.0)\n",
            "Downloading mediapipe-0.10.21-cp311-cp311-manylinux_2_28_x86_64.whl (35.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.6/35.6 MB\u001b[0m \u001b[31m31.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m55.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading protobuf-4.25.6-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.6/294.6 kB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sounddevice-0.5.1-py3-none-any.whl (32 kB)\n",
            "Installing collected packages: protobuf, numpy, sounddevice, mediapipe\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 5.29.4\n",
            "    Uninstalling protobuf-5.29.4:\n",
            "      Successfully uninstalled protobuf-5.29.4\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ydf 0.11.0 requires protobuf<6.0.0,>=5.29.1, but you have protobuf 4.25.6 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\n",
            "grpcio-status 1.71.0 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 4.25.6 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed mediapipe-0.10.21 numpy-1.26.4 protobuf-4.25.6 sounddevice-0.5.1\n"
          ]
        }
      ],
      "source": [
        "!pip install mediapipe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CK1s8XaQmB48",
        "outputId": "0836ef0b-b66d-47c3-9497-bc2d0793c452"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: Flask in /usr/local/lib/python3.11/dist-packages (3.1.0)\n",
            "Collecting flask-ngrok\n",
            "  Downloading flask_ngrok-0.0.25-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: Werkzeug>=3.1 in /usr/local/lib/python3.11/dist-packages (from Flask) (3.1.3)\n",
            "Requirement already satisfied: Jinja2>=3.1.2 in /usr/local/lib/python3.11/dist-packages (from Flask) (3.1.6)\n",
            "Requirement already satisfied: itsdangerous>=2.2 in /usr/local/lib/python3.11/dist-packages (from Flask) (2.2.0)\n",
            "Requirement already satisfied: click>=8.1.3 in /usr/local/lib/python3.11/dist-packages (from Flask) (8.1.8)\n",
            "Requirement already satisfied: blinker>=1.9 in /usr/local/lib/python3.11/dist-packages (from Flask) (1.9.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from flask-ngrok) (2.32.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from Jinja2>=3.1.2->Flask) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->flask-ngrok) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->flask-ngrok) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->flask-ngrok) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->flask-ngrok) (2025.1.31)\n",
            "Downloading flask_ngrok-0.0.25-py3-none-any.whl (3.1 kB)\n",
            "Installing collected packages: flask-ngrok\n",
            "Successfully installed flask-ngrok-0.0.25\n",
            "Collecting pytesseract\n",
            "  Downloading pytesseract-0.3.13-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.11/dist-packages (from pytesseract) (24.2)\n",
            "Requirement already satisfied: Pillow>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from pytesseract) (11.1.0)\n",
            "Downloading pytesseract-0.3.13-py3-none-any.whl (14 kB)\n",
            "Installing collected packages: pytesseract\n",
            "Successfully installed pytesseract-0.3.13\n",
            "Collecting langdetect\n",
            "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from langdetect) (1.17.0)\n",
            "Building wheels for collected packages: langdetect\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993223 sha256=dc0ce132c162d476bfc414f4cbe978c3dec83a003742beef2bdd8e85c0cc1ace\n",
            "  Stored in directory: /root/.cache/pip/wheels/0a/f2/b2/e5ca405801e05eb7c8ed5b3b4bcf1fcabcd6272c167640072e\n",
            "Successfully built langdetect\n",
            "Installing collected packages: langdetect\n",
            "Successfully installed langdetect-1.0.9\n",
            "Requirement already satisfied: gTTS in /usr/local/lib/python3.11/dist-packages (2.5.4)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.11/dist-packages (from gTTS) (2.32.3)\n",
            "Requirement already satisfied: click<8.2,>=7.1 in /usr/local/lib/python3.11/dist-packages (from gTTS) (8.1.8)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->gTTS) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->gTTS) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->gTTS) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->gTTS) (2025.1.31)\n",
            "Collecting googletrans==4.0.0-rc1\n",
            "  Downloading googletrans-4.0.0rc1.tar.gz (20 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting httpx==0.13.3 (from googletrans==4.0.0-rc1)\n",
            "  Downloading httpx-0.13.3-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (2025.1.31)\n",
            "Collecting hstspreload (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading hstspreload-2025.1.1-py3-none-any.whl.metadata (2.1 kB)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (1.3.1)\n",
            "Collecting chardet==3.* (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading chardet-3.0.4-py2.py3-none-any.whl.metadata (3.2 kB)\n",
            "Collecting idna==2.* (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading idna-2.10-py2.py3-none-any.whl.metadata (9.1 kB)\n",
            "Collecting rfc3986<2,>=1.3 (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading rfc3986-1.5.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting httpcore==0.9.* (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading httpcore-0.9.1-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting h11<0.10,>=0.8 (from httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading h11-0.9.0-py2.py3-none-any.whl.metadata (8.1 kB)\n",
            "Collecting h2==3.* (from httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading h2-3.2.0-py2.py3-none-any.whl.metadata (32 kB)\n",
            "Collecting hyperframe<6,>=5.2.0 (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading hyperframe-5.2.0-py2.py3-none-any.whl.metadata (7.2 kB)\n",
            "Collecting hpack<4,>=3.0 (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading hpack-3.0.0-py2.py3-none-any.whl.metadata (7.0 kB)\n",
            "Downloading httpx-0.13.3-py3-none-any.whl (55 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.1/55.1 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading chardet-3.0.4-py2.py3-none-any.whl (133 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.4/133.4 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpcore-0.9.1-py3-none-any.whl (42 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading idna-2.10-py2.py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading h2-3.2.0-py2.py3-none-any.whl (65 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.0/65.0 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rfc3986-1.5.0-py2.py3-none-any.whl (31 kB)\n",
            "Downloading hstspreload-2025.1.1-py3-none-any.whl (1.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m30.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading h11-0.9.0-py2.py3-none-any.whl (53 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.6/53.6 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading hpack-3.0.0-py2.py3-none-any.whl (38 kB)\n",
            "Downloading hyperframe-5.2.0-py2.py3-none-any.whl (12 kB)\n",
            "Building wheels for collected packages: googletrans\n",
            "  Building wheel for googletrans (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for googletrans: filename=googletrans-4.0.0rc1-py3-none-any.whl size=17396 sha256=e682f5fcd67da8f6b8bc0fb328b4821e45b72f50ecfd324d7213798b64c36ec4\n",
            "  Stored in directory: /root/.cache/pip/wheels/39/17/6f/66a045ea3d168826074691b4b787b8f324d3f646d755443fda\n",
            "Successfully built googletrans\n",
            "Installing collected packages: rfc3986, hyperframe, hpack, h11, chardet, idna, hstspreload, h2, httpcore, httpx, googletrans\n",
            "  Attempting uninstall: hyperframe\n",
            "    Found existing installation: hyperframe 6.1.0\n",
            "    Uninstalling hyperframe-6.1.0:\n",
            "      Successfully uninstalled hyperframe-6.1.0\n",
            "  Attempting uninstall: hpack\n",
            "    Found existing installation: hpack 4.1.0\n",
            "    Uninstalling hpack-4.1.0:\n",
            "      Successfully uninstalled hpack-4.1.0\n",
            "  Attempting uninstall: h11\n",
            "    Found existing installation: h11 0.14.0\n",
            "    Uninstalling h11-0.14.0:\n",
            "      Successfully uninstalled h11-0.14.0\n",
            "  Attempting uninstall: chardet\n",
            "    Found existing installation: chardet 5.2.0\n",
            "    Uninstalling chardet-5.2.0:\n",
            "      Successfully uninstalled chardet-5.2.0\n",
            "  Attempting uninstall: idna\n",
            "    Found existing installation: idna 3.10\n",
            "    Uninstalling idna-3.10:\n",
            "      Successfully uninstalled idna-3.10\n",
            "  Attempting uninstall: h2\n",
            "    Found existing installation: h2 4.2.0\n",
            "    Uninstalling h2-4.2.0:\n",
            "      Successfully uninstalled h2-4.2.0\n",
            "  Attempting uninstall: httpcore\n",
            "    Found existing installation: httpcore 1.0.7\n",
            "    Uninstalling httpcore-1.0.7:\n",
            "      Successfully uninstalled httpcore-1.0.7\n",
            "  Attempting uninstall: httpx\n",
            "    Found existing installation: httpx 0.28.1\n",
            "    Uninstalling httpx-0.28.1:\n",
            "      Successfully uninstalled httpx-0.28.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "langsmith 0.3.28 requires httpx<1,>=0.23.0, but you have httpx 0.13.3 which is incompatible.\n",
            "openai 1.72.0 requires httpx<1,>=0.23.0, but you have httpx 0.13.3 which is incompatible.\n",
            "google-genai 1.10.0 requires httpx<1.0.0,>=0.28.1, but you have httpx 0.13.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed chardet-3.0.4 googletrans-4.0.0rc1 h11-0.9.0 h2-3.2.0 hpack-3.0.0 hstspreload-2025.1.1 httpcore-0.9.1 httpx-0.13.3 hyperframe-5.2.0 idna-2.10 rfc3986-1.5.0\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (11.1.0)\n",
            "Requirement already satisfied: pygame in /usr/local/lib/python3.11/dist-packages (2.6.1)\n",
            "Requirement already satisfied: flask-cors in /usr/local/lib/python3.11/dist-packages (5.0.1)\n",
            "Requirement already satisfied: flask>=0.9 in /usr/local/lib/python3.11/dist-packages (from flask-cors) (3.1.0)\n",
            "Requirement already satisfied: Werkzeug>=0.7 in /usr/local/lib/python3.11/dist-packages (from flask-cors) (3.1.3)\n",
            "Requirement already satisfied: Jinja2>=3.1.2 in /usr/local/lib/python3.11/dist-packages (from flask>=0.9->flask-cors) (3.1.6)\n",
            "Requirement already satisfied: itsdangerous>=2.2 in /usr/local/lib/python3.11/dist-packages (from flask>=0.9->flask-cors) (2.2.0)\n",
            "Requirement already satisfied: click>=8.1.3 in /usr/local/lib/python3.11/dist-packages (from flask>=0.9->flask-cors) (8.1.8)\n",
            "Requirement already satisfied: blinker>=1.9 in /usr/local/lib/python3.11/dist-packages (from flask>=0.9->flask-cors) (1.9.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from Werkzeug>=0.7->flask-cors) (3.0.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install Flask flask-ngrok\n",
        "!pip install pytesseract\n",
        "!pip install langdetect\n",
        "!pip install gTTS\n",
        "!pip install googletrans==4.0.0-rc1\n",
        "!pip install pillow\n",
        "!pip install pygame\n",
        "!pip install flask-cors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5vfhhLS2mFzp",
        "outputId": "48f6ec44-52a8-4632-91d9-2051ef50f566"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rHit:1 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "\r0% [Waiting for headers] [Waiting for headers] [Connecting to cloud.r-project.o\r                                                                               \rGet:2 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "\r                                                                               \rGet:3 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "\r0% [2 InRelease 15.6 kB/128 kB 12%] [3 InRelease 31.5 kB/129 kB 24%] [Connectin\r0% [2 InRelease 73.5 kB/128 kB 57%] [Connecting to cloud.r-project.org] [Connec\r0% [Waiting for headers] [Connecting to cloud.r-project.org] [Waiting for heade\r                                                                               \rHit:4 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Get:5 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "Get:6 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\n",
            "Hit:7 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Get:8 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,243 kB]\n",
            "Hit:9 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Get:11 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [2,788 kB]\n",
            "Get:12 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,542 kB]\n",
            "Hit:13 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:14 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [4,161 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [3,101 kB]\n",
            "Get:16 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,697 kB]\n",
            "Get:17 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ Packages [73.0 kB]\n",
            "Get:18 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [8,847 kB]\n",
            "Fetched 24.7 MB in 3s (9,257 kB/s)\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "tesseract-ocr is already the newest version (4.1.1-2.1build1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 47 not upgraded.\n"
          ]
        }
      ],
      "source": [
        "!apt-get update\n",
        "!apt-get install -y tesseract-ocr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 470
        },
        "id": "-T1vwCT2Xvod",
        "outputId": "f981187e-6a59-464b-8a15-11ec74cce491"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Creating new Ultralytics Settings v0.0.6 file ✅ \n",
            "View Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\n",
            "Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n",
            "25-03-25 08:53:12 - Directory /root/.deepface has been created\n",
            "25-03-25 08:53:12 - Directory /root/.deepface/weights has been created\n"
          ]
        },
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'mediapipe'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-0169b6ac09b7>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyngrok\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mngrok\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtempfile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mmediapipe\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpytesseract\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlangdetect\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdetect\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'mediapipe'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "# from flask import Flask, request, jsonify\n",
        "# from flask_cors import CORS\n",
        "# import cv2\n",
        "# import numpy as np\n",
        "# import torch\n",
        "# from gtts import gTTS\n",
        "# import time\n",
        "# from threading import Thread\n",
        "# from ultralytics import YOLO\n",
        "# from deepface import DeepFace\n",
        "# from io import BytesIO\n",
        "# from PIL import Image\n",
        "# import base64\n",
        "# from pyngrok import ngrok\n",
        "# import tempfile\n",
        "# import mediapipe as mp\n",
        "# import pytesseract\n",
        "# from langdetect import detect\n",
        "# from googletrans import Translator\n",
        "# import os\n",
        "\n",
        "# # Initialize Flask app\n",
        "# app = Flask(__name__)\n",
        "# CORS(app)\n",
        "\n",
        "# # Load models\n",
        "\n",
        "# pytesseract.pytesseract.tesseract_cmd = r\"/usr/bin/tesseract\"\n",
        "# yolo_model = YOLO(\"yolov8n.pt\")  # General object detection\n",
        "# currency_model = YOLO(\"/content/drive/MyDrive/best.pt\")  # Custom-trained currency detection\n",
        "\n",
        "# # Initialize MediaPipe Hands\n",
        "# mp_hands = mp.solutions.hands\n",
        "# hands = mp_hands.Hands(min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
        "# mp_draw = mp.solutions.drawing_utils\n",
        "\n",
        "# # Start ngrok\n",
        "# ngrok_tunnel = ngrok.connect(5000)\n",
        "# print(f\"Public URL: {ngrok_tunnel.public_url}\")\n",
        "\n",
        "# # Track last spoken time\n",
        "# last_spoken_time = time.time()\n",
        "\n",
        "# # Control flags\n",
        "# yolo_emotion_enabled = False\n",
        "# tts_enabled = False\n",
        "\n",
        "# def count_fingers(hand_landmarks):\n",
        "#     \"\"\"Count the number of extended fingers.\"\"\"\n",
        "#     finger_tips = [4, 8, 12, 16, 20]\n",
        "#     extended_fingers = sum(\n",
        "#         1 for tip in finger_tips if hand_landmarks.landmark[tip].y < hand_landmarks.landmark[tip - 2].y\n",
        "#     )\n",
        "#     return extended_fingers\n",
        "\n",
        "# def generate_audio(text):\n",
        "#     \"\"\"Generate speech audio using gTTS and return Base64-encoded MP3.\"\"\"\n",
        "#     tts = gTTS(text=text, lang='en')\n",
        "\n",
        "#     with tempfile.NamedTemporaryFile(delete=True, suffix=\".mp3\") as temp_audio:\n",
        "#         tts.save(temp_audio.name)\n",
        "\n",
        "#         # Read and encode the audio file\n",
        "#         with open(temp_audio.name, \"rb\") as f:\n",
        "#             audio_base64 = base64.b64encode(f.read()).decode(\"utf-8\")\n",
        "\n",
        "#     return audio_base64\n",
        "\n",
        "# def preprocess_frame(frame):\n",
        "#     \"\"\"Preprocess the frame for TTS functionality.\"\"\"\n",
        "#     gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "#     upscale_factor = 4\n",
        "#     upscaled = cv2.resize(gray, (gray.shape[1] * upscale_factor, gray.shape[0] * upscale_factor))\n",
        "#     denoised = cv2.medianBlur(upscaled, 5)\n",
        "#     blurred = cv2.GaussianBlur(denoised, (9, 9), 0)\n",
        "#     thresh = cv2.adaptiveThreshold(blurred, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 35, 2)\n",
        "#     kernel = np.ones((1, 1), np.uint8)\n",
        "#     dilated = cv2.dilate(thresh, kernel, iterations=1)\n",
        "#     return dilated\n",
        "\n",
        "# def process_image(image_path):\n",
        "#     \"\"\"Process the image and extract text for TTS functionality.\"\"\"\n",
        "#     try:\n",
        "#         img = Image.open(image_path)\n",
        "#         raw_text_sample = pytesseract.image_to_string(img, lang=\"hin+tam+tel+eng\").strip()\n",
        "#         detected_lang = detect(raw_text_sample) if raw_text_sample else \"unknown\"\n",
        "\n",
        "#         extracted_text = pytesseract.image_to_string(img, lang=\"hin+tam+tel+eng\").strip()\n",
        "#         if not extracted_text:\n",
        "#             return None, None, None\n",
        "\n",
        "#         translator = Translator()\n",
        "#         translated_text = extracted_text\n",
        "#         if detected_lang != \"en\":\n",
        "#             try:\n",
        "#                 translation_result = translator.translate(extracted_text, src=detected_lang, dest=\"en\")\n",
        "#                 translated_text = translation_result.text if translation_result else \"Translation failed\"\n",
        "#             except Exception as e:\n",
        "#                 translated_text = \"Translation failed\"\n",
        "\n",
        "#         processed_image_path = \"processed_image.jpg\"\n",
        "#         img.save(processed_image_path)\n",
        "\n",
        "#         return extracted_text, translated_text, processed_image_path\n",
        "\n",
        "#     except Exception as e:\n",
        "#         return None, None, None\n",
        "\n",
        "\n",
        "# @app.route('/detect', methods=['POST'])\n",
        "# def detect_objects_and_emotions():\n",
        "#     global last_spoken_time, yolo_emotion_enabled, tts_enabled\n",
        "\n",
        "\n",
        "#     try:\n",
        "#         img_data = request.json['image']\n",
        "#         img_bytes = base64.b64decode(img_data)\n",
        "#         img = Image.open(BytesIO(img_bytes)).convert(\"RGB\")\n",
        "\n",
        "#         img_np = np.array(img)\n",
        "#         img_cv = cv2.cvtColor(img_np, cv2.COLOR_RGB2BGR)\n",
        "\n",
        "#         rgb_img = cv2.cvtColor(img_cv, cv2.COLOR_BGR2RGB)\n",
        "#         result = hands.process(rgb_img)\n",
        "\n",
        "#         gesture_text = \"No Change\"\n",
        "#         if result.multi_hand_landmarks:\n",
        "#             for hand_landmarks in result.multi_hand_landmarks:\n",
        "#                 mp_draw.draw_landmarks(img_cv, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
        "#                 fingers = count_fingers(hand_landmarks)\n",
        "\n",
        "#                 if fingers == 1:\n",
        "#                     yolo_emotion_enabled = True\n",
        "#                     tts_enabled = False\n",
        "#                     gesture_text = \"YOLO Emotion Enabled\"\n",
        "\n",
        "#                 elif fingers == 3:\n",
        "#                     yolo_emotion_enabled = False\n",
        "#                     tts_enabled = True\n",
        "#                     gesture_text = \"TTS Enabled\"\n",
        "\n",
        "#                 cv2.putText(img_cv, f\"Gesture: {gesture_text}\", (50, 100), cv2.FONT_HERSHEY_SIMPLEX,\n",
        "#                             1, (0, 255, 0), 2, cv2.LINE_AA)\n",
        "\n",
        "#         response_data = {\n",
        "#             \"gesture\": gesture_text,\n",
        "#             \"processed_image\": \"\",\n",
        "#             \"audio\": \"\",\n",
        "#         }\n",
        "\n",
        "#         if yolo_emotion_enabled:\n",
        "#             results = yolo_model(img_cv)[0]\n",
        "#             detected_objects = []\n",
        "\n",
        "#             for box in results.boxes:\n",
        "#                 x1, y1, x2, y2 = map(int, box.xyxy[0])\n",
        "#                 cls = int(box.cls[0])\n",
        "#                 label = yolo_model.names[cls]\n",
        "#                 detected_objects.append(label)\n",
        "\n",
        "#             emotion_result = DeepFace.analyze(img_cv, actions=['emotion'], enforce_detection=False, detector_backend='opencv')[0]\n",
        "#             emotion_text = emotion_result['dominant_emotion']\n",
        "#             speech_text = f\"Emotion detected: {emotion_text}. Objects: {', '.join(detected_objects)}.\"\n",
        "\n",
        "#             current_time = time.time()\n",
        "#             if current_time - last_spoken_time >= 15:\n",
        "#                 last_spoken_time = current_time\n",
        "#                 response_data[\"audio\"] = generate_audio(speech_text)\n",
        "\n",
        "#         elif tts_enabled:\n",
        "#             image_path = \"temp_frame.jpg\"\n",
        "#             cv2.imwrite(image_path, img_cv)\n",
        "#             extracted_text, translated_text, processed_image_path = process_image(image_path)\n",
        "\n",
        "#             if translated_text:\n",
        "#                 response_data[\"audio\"] = generate_audio(translated_text)\n",
        "\n",
        "#         _, buffer = cv2.imencode('.jpg', img_cv)\n",
        "#         response_data[\"processed_image\"] = base64.b64encode(buffer).decode('utf-8')\n",
        "\n",
        "#         return jsonify(response_data)\n",
        "\n",
        "#     except Exception as e:\n",
        "#         return jsonify({\"error\": str(e)})\n",
        "\n",
        "\n",
        "# if __name__ == '__main__':\n",
        "#     app.run(port=5000)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w-vWsiBl5vX2",
        "outputId": "096aab3b-7a53-4c7a-a4e6-caa4c5fb4f67"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: google-generativeai in /usr/local/lib/python3.11/dist-packages (0.8.4)\n",
            "Requirement already satisfied: google-ai-generativelanguage==0.6.15 in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (0.6.15)\n",
            "Requirement already satisfied: google-api-core in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.24.2)\n",
            "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.164.0)\n",
            "Requirement already satisfied: google-auth>=2.15.0 in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.38.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (5.29.4)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.11.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (4.13.1)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage==0.6.15->google-generativeai) (1.26.1)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core->google-generativeai) (1.69.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.18.0 in /usr/local/lib/python3.11/dist-packages (from google-api-core->google-generativeai) (2.32.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (4.9)\n",
            "Requirement already satisfied: httplib2<1.dev0,>=0.19.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (0.22.0)\n",
            "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (0.2.0)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (4.1.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic->google-generativeai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic->google-generativeai) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic->google-generativeai) (0.4.0)\n",
            "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.71.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.71.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.11/dist-packages (from httplib2<1.dev0,>=0.19.0->google-api-python-client->google-generativeai) (3.2.3)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai) (0.6.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (2.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (2025.1.31)\n"
          ]
        }
      ],
      "source": [
        "!pip install google-generativeai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 954
        },
        "id": "9UJtRZglzp-B",
        "outputId": "68583609-d84b-47cd-a185-5403906bdf69"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Public URL: https://bc16-34-90-153-149.ngrok-free.app\n",
            " * Serving Flask app '__main__'\n",
            " * Debug mode: off\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
            " * Running on http://127.0.0.1:5000\n",
            "INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n",
            "INFO:werkzeug:127.0.0.1 - - [25/Mar/2025 06:07:00] \"POST /detect HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [25/Mar/2025 06:07:02] \"POST /detect HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [25/Mar/2025 06:07:06] \"POST /detect HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [25/Mar/2025 06:07:10] \"POST /detect HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [25/Mar/2025 06:07:13] \"POST /detect HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [25/Mar/2025 06:07:16] \"POST /detect HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [25/Mar/2025 06:07:19] \"POST /detect HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [25/Mar/2025 06:07:22] \"POST /detect HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [25/Mar/2025 06:07:27] \"POST /detect HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [25/Mar/2025 06:07:31] \"POST /detect HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [25/Mar/2025 06:07:34] \"POST /detect HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "0: 480x640 2 persons, 1 tie, 213.7ms\n",
            "Speed: 5.0ms preprocess, 213.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [25/Mar/2025 06:07:37] \"POST /detect HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "0: 480x640 1 person, 206.2ms\n",
            "Speed: 5.4ms preprocess, 206.2ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [25/Mar/2025 06:07:41] \"POST /detect HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "0: 480x640 1 person, 194.2ms\n",
            "Speed: 5.6ms preprocess, 194.2ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [25/Mar/2025 06:07:43] \"POST /detect HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [25/Mar/2025 06:07:47] \"POST /detect HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [25/Mar/2025 06:07:51] \"POST /detect HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [25/Mar/2025 06:07:55] \"POST /detect HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [25/Mar/2025 06:07:58] \"POST /detect HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [25/Mar/2025 06:08:02] \"POST /detect HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [25/Mar/2025 06:08:05] \"POST /detect HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [25/Mar/2025 06:08:09] \"POST /detect HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [25/Mar/2025 06:08:13] \"POST /detect HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [25/Mar/2025 06:08:17] \"POST /detect HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [25/Mar/2025 06:08:20] \"POST /detect HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [25/Mar/2025 06:08:23] \"POST /detect HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [25/Mar/2025 06:08:26] \"POST /detect HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [25/Mar/2025 06:08:29] \"POST /detect HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "0: 480x640 1 person, 199.3ms\n",
            "Speed: 3.3ms preprocess, 199.3ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [25/Mar/2025 06:08:32] \"POST /detect HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "0: 480x640 1 person, 188.8ms\n",
            "Speed: 3.6ms preprocess, 188.8ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [25/Mar/2025 06:08:35] \"POST /detect HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "0: 480x640 1 person, 189.7ms\n",
            "Speed: 3.3ms preprocess, 189.7ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [25/Mar/2025 06:08:37] \"POST /detect HTTP/1.1\" 200 -\n"
          ]
        }
      ],
      "source": [
        "# from flask import Flask, request, jsonify\n",
        "# from flask_cors import CORS\n",
        "# import cv2\n",
        "# import numpy as np\n",
        "# import torch\n",
        "# from gtts import gTTS\n",
        "# import time\n",
        "# from threading import Thread\n",
        "# from ultralytics import YOLO\n",
        "# from deepface import DeepFace\n",
        "# from io import BytesIO\n",
        "# from PIL import Image\n",
        "# import base64\n",
        "# from pyngrok import ngrok\n",
        "# import tempfile\n",
        "# import mediapipe as mp\n",
        "# import pytesseract\n",
        "# from langdetect import detect\n",
        "# from googletrans import Translator\n",
        "# from textblob import TextBlob  # Import TextBlob for spell checking\n",
        "# import os\n",
        "# import random\n",
        "# import google.generativeai as genai\n",
        "# import re\n",
        "\n",
        "\n",
        "# # Initialize Flask app\n",
        "# app = Flask(__name__)\n",
        "# CORS(app)\n",
        "\n",
        "# # Load models\n",
        "# pytesseract.pytesseract.tesseract_cmd = r\"/usr/bin/tesseract\"\n",
        "# yolo_model = YOLO(\"yolov8n.pt\")  # General object detection\n",
        "# currency_model = YOLO(\"/content/drive/MyDrive/best.pt\")  # Custom-trained currency detection\n",
        "\n",
        "# # Initialize MediaPipe Hands\n",
        "# mp_hands = mp.solutions.hands\n",
        "# hands = mp_hands.Hands(min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
        "# mp_draw = mp.solutions.drawing_utils\n",
        "\n",
        "# # Start ngrok\n",
        "# ngrok_tunnel = ngrok.connect(5000)\n",
        "# print(f\"Public URL: {ngrok_tunnel.public_url}\")\n",
        "\n",
        "# # Track last spoken time\n",
        "# last_spoken_time = time.time()\n",
        "\n",
        "# # Control flags\n",
        "# yolo_emotion_enabled = False\n",
        "# tts_enabled = False\n",
        "\n",
        "\n",
        "# def count_fingers(hand_landmarks):\n",
        "#     \"\"\"Count the number of extended fingers.\"\"\"\n",
        "#     finger_tips = [4, 8, 12, 16, 20]\n",
        "#     extended_fingers = sum(\n",
        "#         1 for tip in finger_tips if hand_landmarks.landmark[tip].y < hand_landmarks.landmark[tip - 2].y\n",
        "#     )\n",
        "#     return extended_fingers\n",
        "\n",
        "\n",
        "# def generate_audio(text):\n",
        "#     \"\"\"Generate speech audio using gTTS and return Base64-encoded MP3.\"\"\"\n",
        "#     tts = gTTS(text=text, lang='en')\n",
        "\n",
        "#     with tempfile.NamedTemporaryFile(delete=True, suffix=\".mp3\") as temp_audio:\n",
        "#         tts.save(temp_audio.name)\n",
        "\n",
        "#         # Read and encode the audio file\n",
        "#         with open(temp_audio.name, \"rb\") as f:\n",
        "#             audio_base64 = base64.b64encode(f.read()).decode(\"utf-8\")\n",
        "\n",
        "#     return audio_base64\n",
        "\n",
        "\n",
        "# def preprocess_frame(frame):\n",
        "#     \"\"\"Preprocess the frame for TTS functionality.\"\"\"\n",
        "#     gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "#     upscale_factor = 4\n",
        "#     upscaled = cv2.resize(gray, (gray.shape[1] * upscale_factor, gray.shape[0] * upscale_factor))\n",
        "#     denoised = cv2.medianBlur(upscaled, 5)\n",
        "#     blurred = cv2.GaussianBlur(denoised, (9, 9), 0)\n",
        "#     thresh = cv2.adaptiveThreshold(blurred, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 35, 2)\n",
        "#     kernel = np.ones((1, 1), np.uint8)\n",
        "#     dilated = cv2.dilate(thresh, kernel, iterations=1)\n",
        "#     return dilated\n",
        "\n",
        "\n",
        "# def process_image(image_path):\n",
        "#     \"\"\"Process the image and extract text for TTS functionality using Gemini for spell-checking and enhancement.\"\"\"\n",
        "#     try:\n",
        "#         # Extract text from the image using Tesseract\n",
        "#         img = Image.open(image_path)\n",
        "#         raw_text_sample = pytesseract.image_to_string(img, lang=\"hin+tam+tel+eng\").strip()\n",
        "#         detected_lang = detect(raw_text_sample) if raw_text_sample else \"unknown\"\n",
        "\n",
        "#         # Use Gemini to spell-check and enhance the text\n",
        "#         genai.configure(api_key=\"AIzaSyBw8BSw6ea6ZdJ9Ck77TXSONiHYO7q4VnI\")\n",
        "#         model = genai.GenerativeModel(model_name=\"gemini-1.5-flash\")\n",
        "#         response = model.generate_content(\n",
        "#             f\"Improve and spell-check this text: '{raw_text_sample}'. Keep it concise and accurate.\"\n",
        "#         )\n",
        "#         corrected_text = response.text if response.text else raw_text_sample  # Use Gemini's output or fallback to raw text\n",
        "\n",
        "#         # Translate the text if necessary\n",
        "#         translator = Translator()\n",
        "#         translated_text = corrected_text\n",
        "#         if detected_lang != \"en\":\n",
        "#             try:\n",
        "#                 translation_result = translator.translate(corrected_text, src=detected_lang, dest=\"en\")\n",
        "#                 translated_text = translation_result.text if translation_result else \"Translation failed\"\n",
        "#             except Exception as e:\n",
        "#                 translated_text = \"\"\n",
        "\n",
        "#         processed_image_path = \"processed_image.jpg\"\n",
        "#         img.save(processed_image_path)\n",
        "\n",
        "#         return corrected_text, translated_text, processed_image_path\n",
        "\n",
        "#     except Exception as e:\n",
        "#         return None, None, None\n",
        "\n",
        "\n",
        "\n",
        "# @app.route('/detect', methods=['POST'])\n",
        "# def detect_objects_and_emotions():\n",
        "#     global last_spoken_time, yolo_emotion_enabled, tts_enabled\n",
        "\n",
        "#     try:\n",
        "#         img_data = request.json['image']\n",
        "#         img_bytes = base64.b64decode(img_data)\n",
        "#         img = Image.open(BytesIO(img_bytes)).convert(\"RGB\")\n",
        "\n",
        "#         img_np = np.array(img)\n",
        "#         img_cv = cv2.cvtColor(img_np, cv2.COLOR_RGB2BGR)\n",
        "\n",
        "#         rgb_img = cv2.cvtColor(img_cv, cv2.COLOR_BGR2RGB)\n",
        "#         result = hands.process(rgb_img)\n",
        "\n",
        "#         gesture_text = \"No Change\"\n",
        "#         if result.multi_hand_landmarks:\n",
        "#             for hand_landmarks in result.multi_hand_landmarks:\n",
        "#                 mp_draw.draw_landmarks(img_cv, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
        "#                 fingers = count_fingers(hand_landmarks)\n",
        "\n",
        "#                 if fingers == 1:\n",
        "#                     yolo_emotion_enabled = True\n",
        "#                     tts_enabled = False\n",
        "#                     gesture_text = \"YOLO Emotion Enabled\"\n",
        "\n",
        "#                 elif fingers == 3:\n",
        "#                     yolo_emotion_enabled = False\n",
        "#                     tts_enabled = True\n",
        "#                     gesture_text = \"TTS Enabled\"\n",
        "\n",
        "#                 cv2.putText(img_cv, f\"Gesture: {gesture_text}\", (50, 100), cv2.FONT_HERSHEY_SIMPLEX,\n",
        "#                             1, (0, 255, 0), 2, cv2.LINE_AA)\n",
        "\n",
        "#         response_data = {\n",
        "#             \"gesture\": gesture_text,\n",
        "#             \"processed_image\": \"\",\n",
        "#             \"audio\": \"\",\n",
        "#         }\n",
        "\n",
        "#         if yolo_emotion_enabled:\n",
        "#             results = yolo_model(img_cv)[0]\n",
        "#             detected_objects = []\n",
        "\n",
        "#             for box in results.boxes:\n",
        "#                 x1, y1, x2, y2 = map(int, box.xyxy[0])\n",
        "#                 cls = int(box.cls[0])\n",
        "#                 label = yolo_model.names[cls]\n",
        "#                 detected_objects.append(label)\n",
        "\n",
        "#             emotion_result = DeepFace.analyze(img_cv, actions=['emotion'], enforce_detection=False, detector_backend='opencv')[0]\n",
        "#             emotion_text = emotion_result['dominant_emotion']\n",
        "#             speech_text = f\"Emotion detected: {emotion_text}. Objects: {', '.join(detected_objects)}.\"\n",
        "\n",
        "#             current_time = time.time()\n",
        "#             if current_time - last_spoken_time >= 15:\n",
        "#                 last_spoken_time = current_time\n",
        "#                 response_data[\"audio\"] = generate_audio(speech_text)\n",
        "\n",
        "#         elif tts_enabled:\n",
        "#             image_path = \"temp_frame.jpg\"\n",
        "#             cv2.imwrite(image_path, img_cv)\n",
        "#             extracted_text, translated_text, processed_image_path = process_image(image_path)\n",
        "\n",
        "#             if translated_text:\n",
        "#                 response_data[\"audio\"] = generate_audio(translated_text)\n",
        "\n",
        "#         _, buffer = cv2.imencode('.jpg', img_cv)\n",
        "#         response_data[\"processed_image\"] = base64.b64encode(buffer).decode('utf-8')\n",
        "\n",
        "#         return jsonify(response_data)\n",
        "\n",
        "#     except Exception as e:\n",
        "#         return jsonify({\"error\": str(e)})\n",
        "\n",
        "\n",
        "# if __name__ == '__main__':\n",
        "#     app.run(port=5000)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "i0OX-sfz7m1_",
        "outputId": "98bbd764-5b80-4db2-dc18-c33e7063eac7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Public URL: https://b737-35-185-47-57.ngrok-free.app\n",
            " * Serving Flask app '__main__'\n",
            " * Debug mode: off\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
            " * Running on http://127.0.0.1:5000\n",
            "INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n",
            "INFO:werkzeug:127.0.0.1 - - [26/Mar/2025 06:34:56] \"POST /detect HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "0: 480x640 2 persons, 1 tie, 174.9ms\n",
            "Speed: 6.0ms preprocess, 174.9ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [26/Mar/2025 06:35:03] \"POST /detect HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "0: 480x640 1 person, 1 remote, 164.9ms\n",
            "Speed: 5.6ms preprocess, 164.9ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [26/Mar/2025 06:35:14] \"POST /detect HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "0: 480x640 1 person, 175.9ms\n",
            "Speed: 4.2ms preprocess, 175.9ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [26/Mar/2025 06:35:21] \"POST /detect HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "0: 480x640 1 person, 280.7ms\n",
            "Speed: 3.3ms preprocess, 280.7ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [26/Mar/2025 06:35:38] \"POST /detect HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "0: 480x640 3 persons, 1 cell phone, 199.3ms\n",
            "Speed: 6.2ms preprocess, 199.3ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [26/Mar/2025 06:35:59] \"POST /detect HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "0: 480x640 (no detections), 181.5ms\n",
            "Speed: 3.1ms preprocess, 181.5ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [26/Mar/2025 06:36:22] \"POST /detect HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "0: 480x640 1 person, 170.1ms\n",
            "Speed: 3.5ms preprocess, 170.1ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [26/Mar/2025 06:36:32] \"POST /detect HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "0: 480x640 1 person, 191.2ms\n",
            "Speed: 6.8ms preprocess, 191.2ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 640)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [26/Mar/2025 06:36:38] \"POST /detect HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "0: 480x640 1 person, 175.2ms\n",
            "Speed: 5.4ms preprocess, 175.2ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [26/Mar/2025 06:36:49] \"POST /detect HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "0: 480x640 1 person, 177.0ms\n",
            "Speed: 6.6ms preprocess, 177.0ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [26/Mar/2025 06:36:55] \"POST /detect HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "0: 480x640 2 persons, 1 refrigerator, 180.1ms\n",
            "Speed: 5.4ms preprocess, 180.1ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [26/Mar/2025 06:37:08] \"POST /detect HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "0: 480x640 2 persons, 187.1ms\n",
            "Speed: 5.2ms preprocess, 187.1ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [26/Mar/2025 06:37:13] \"POST /detect HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "0: 480x640 1 bed, 265.0ms\n",
            "Speed: 8.2ms preprocess, 265.0ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [26/Mar/2025 06:37:31] \"POST /detect HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [26/Mar/2025 06:37:49] \"POST /detect HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [26/Mar/2025 06:37:57] \"POST /detect HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [26/Mar/2025 06:38:03] \"POST /detect HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [26/Mar/2025 06:38:11] \"POST /detect HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [26/Mar/2025 06:38:17] \"POST /detect HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [26/Mar/2025 06:38:26] \"POST /detect HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [26/Mar/2025 06:38:57] \"POST /detect HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [26/Mar/2025 06:39:08] \"POST /detect HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [26/Mar/2025 06:39:14] \"POST /detect HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [26/Mar/2025 06:39:20] \"POST /detect HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [26/Mar/2025 06:39:26] \"POST /detect HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [26/Mar/2025 06:39:32] \"POST /detect HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [26/Mar/2025 06:39:38] \"POST /detect HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "0: 480x640 1 person, 178.8ms\n",
            "Speed: 4.0ms preprocess, 178.8ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [26/Mar/2025 06:40:01] \"POST /detect HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "0: 480x640 2 persons, 173.8ms\n",
            "Speed: 5.7ms preprocess, 173.8ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [26/Mar/2025 06:40:10] \"POST /detect HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "0: 480x640 1 person, 162.3ms\n",
            "Speed: 4.4ms preprocess, 162.3ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [26/Mar/2025 06:40:15] \"POST /detect HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [26/Mar/2025 06:40:22] \"POST /detect HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [26/Mar/2025 06:40:29] \"POST /detect HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [26/Mar/2025 06:40:37] \"POST /detect HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [26/Mar/2025 06:40:56] \"POST /detect HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [26/Mar/2025 06:41:02] \"POST /detect HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [26/Mar/2025 06:41:08] \"POST /detect HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [26/Mar/2025 06:41:14] \"POST /detect HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [26/Mar/2025 06:41:19] \"POST /detect HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [26/Mar/2025 06:41:25] \"POST /detect HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [26/Mar/2025 06:41:30] \"POST /detect HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "0: 480x640 2 persons, 273.9ms\n",
            "Speed: 6.2ms preprocess, 273.9ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 640)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [26/Mar/2025 06:41:35] \"POST /detect HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "0: 480x640 1 person, 1 scissors, 271.4ms\n",
            "Speed: 3.2ms preprocess, 271.4ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [26/Mar/2025 06:41:47] \"POST /detect HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [26/Mar/2025 06:41:59] \"POST /detect HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [26/Mar/2025 06:42:22] \"POST /detect HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [26/Mar/2025 06:42:27] \"POST /detect HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [26/Mar/2025 06:42:32] \"POST /detect HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [26/Mar/2025 06:42:37] \"POST /detect HTTP/1.1\" 200 -\n"
          ]
        }
      ],
      "source": [
        "# from flask import Flask, request, jsonify\n",
        "# from flask_cors import CORS\n",
        "# import cv2\n",
        "# import numpy as np\n",
        "# import torch\n",
        "# from gtts import gTTS\n",
        "# import time\n",
        "# from threading import Thread\n",
        "# from ultralytics import YOLO\n",
        "# from deepface import DeepFace\n",
        "# from io import BytesIO\n",
        "# from PIL import Image\n",
        "# import base64\n",
        "# from pyngrok import ngrok\n",
        "# import tempfile\n",
        "# import mediapipe as mp\n",
        "# import pytesseract\n",
        "# from langdetect import detect\n",
        "# from googletrans import Translator\n",
        "# import os\n",
        "# import random\n",
        "# import google.generativeai as genai\n",
        "# import re\n",
        "# from gtts import gTTS\n",
        "# from IPython.display import Audio, display\n",
        "# import tempfile\n",
        "\n",
        "# # Initialize Flask app\n",
        "# app = Flask(__name__)\n",
        "# CORS(app)\n",
        "\n",
        "# # Load models\n",
        "\n",
        "# pytesseract.pytesseract.tesseract_cmd = r\"/usr/bin/tesseract\"\n",
        "# yolo_model = YOLO(\"yolov8n.pt\")  # General object detection\n",
        "# currency_model = YOLO(\"/content/drive/MyDrive/best.pt\")  # Custom-trained currency detection\n",
        "\n",
        "# # Initialize MediaPipe Hands\n",
        "# mp_hands = mp.solutions.hands\n",
        "# hands = mp_hands.Hands(min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
        "# mp_draw = mp.solutions.drawing_utils\n",
        "\n",
        "# # Start ngrok\n",
        "# ngrok_tunnel = ngrok.connect(5000)\n",
        "# print(f\"Public URL: {ngrok_tunnel.public_url}\")\n",
        "\n",
        "# # Track last spoken time\n",
        "# last_spoken_time = time.time()\n",
        "\n",
        "# # Control flags\n",
        "# yolo_emotion_enabled = False\n",
        "# tts_enabled = False\n",
        "\n",
        "# def count_fingers(hand_landmarks):\n",
        "#     \"\"\"Count the number of extended fingers.\"\"\"\n",
        "#     finger_tips = [4, 8, 12, 16, 20]\n",
        "#     extended_fingers = sum(\n",
        "#         1 for tip in finger_tips if hand_landmarks.landmark[tip].y < hand_landmarks.landmark[tip - 2].y\n",
        "#     )\n",
        "#     return extended_fingers\n",
        "\n",
        "# def generate_audio(text):\n",
        "#     \"\"\"Generate speech audio using gTTS and return Base64-encoded MP3.\"\"\"\n",
        "#     tts = gTTS(text=text, lang='en')\n",
        "\n",
        "#     with tempfile.NamedTemporaryFile(delete=True, suffix=\".mp3\") as temp_audio:\n",
        "#         tts.save(temp_audio.name)\n",
        "\n",
        "#         # Read and encode the audio file\n",
        "#         with open(temp_audio.name, \"rb\") as f:\n",
        "#             audio_base64 = base64.b64encode(f.read()).decode(\"utf-8\")\n",
        "\n",
        "#     return audio_base64\n",
        "\n",
        "# def preprocess_frame(frame):\n",
        "#     \"\"\"Preprocess the frame for TTS functionality.\"\"\"\n",
        "#     gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "#     upscale_factor = 4\n",
        "#     upscaled = cv2.resize(gray, (gray.shape[1] * upscale_factor, gray.shape[0] * upscale_factor))\n",
        "#     denoised = cv2.medianBlur(upscaled, 5)\n",
        "#     blurred = cv2.GaussianBlur(denoised, (9, 9), 0)\n",
        "#     thresh = cv2.adaptiveThreshold(blurred, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 35, 2)\n",
        "#     kernel = np.ones((1, 1), np.uint8)\n",
        "#     dilated = cv2.dilate(thresh, kernel, iterations=1)\n",
        "#     return dilated\n",
        "\n",
        "# def process_image(image_path):\n",
        "#     \"\"\"Process the image and extract text for TTS functionality.\"\"\"\n",
        "#     try:\n",
        "#         img = Image.open(image_path)\n",
        "#         raw_text_sample = pytesseract.image_to_string(img, lang=\"hin+tam+tel+eng\").strip()\n",
        "#         detected_lang = detect(raw_text_sample) if raw_text_sample else \"unknown\"\n",
        "\n",
        "#         extracted_text = pytesseract.image_to_string(img, lang=\"hin+tam+tel+eng\").strip()\n",
        "#         # Sample output from the code\n",
        "\n",
        "\n",
        "# # Configure Gemini API\n",
        "#         api_key = \"AIzaSyBw8BSw6ea6ZdJ9Ck77TXSONiHYO7q4VnI\"\n",
        "#         genai.configure(api_key=api_key)\n",
        "\n",
        "# # Feed the selected instruction into Gemini\n",
        "#         model = genai.GenerativeModel(model_name=\"gemini-1.5-flash\")\n",
        "#         response = model.generate_content(f\"I have this output from an ocr, i want to use you to spellcheck it and correct it using your high efficient nlp libraries, just produce the ouput only, it should be accurate, this is the statement: {extracted_text}\")\n",
        "\n",
        "# # Output Gemini's analysis\n",
        "#         gemini_analysis = response.text\n",
        "#         if not extracted_text:\n",
        "#             return None, None, None\n",
        "\n",
        "#         translator = Translator()\n",
        "#         translated_text = gemini_analysis\n",
        "#         if detected_lang != \"en\":\n",
        "#             try:\n",
        "#                 translation_result = translator.translate(gemini_analysis, src=detected_lang, dest=\"en\")\n",
        "#                 translated_text = translation_result.text if translation_result else \"Translation failed\"\n",
        "#             except Exception as e:\n",
        "#                 translated_text = \"Translation failed\"\n",
        "\n",
        "#         processed_image_path = \"processed_image.jpg\"\n",
        "#         img.save(processed_image_path)\n",
        "\n",
        "#         return extracted_text, translated_text, processed_image_path\n",
        "\n",
        "#     except Exception as e:\n",
        "#         return None, None, None\n",
        "\n",
        "\n",
        "# @app.route('/detect', methods=['POST'])\n",
        "# def detect_objects_and_emotions():\n",
        "#     global last_spoken_time, yolo_emotion_enabled, tts_enabled, currency_emotion_enabled\n",
        "\n",
        "\n",
        "#     try:\n",
        "#         img_data = request.json['image']\n",
        "#         img_bytes = base64.b64decode(img_data)\n",
        "#         img = Image.open(BytesIO(img_bytes)).convert(\"RGB\")\n",
        "\n",
        "#         img_np = np.array(img)\n",
        "#         img_cv = cv2.cvtColor(img_np, cv2.COLOR_RGB2BGR)\n",
        "\n",
        "#         rgb_img = cv2.cvtColor(img_cv, cv2.COLOR_BGR2RGB)\n",
        "#         result = hands.process(rgb_img)\n",
        "\n",
        "#         gesture_text = \"No Change\"\n",
        "#         if result.multi_hand_landmarks:\n",
        "#             for hand_landmarks in result.multi_hand_landmarks:\n",
        "#                 mp_draw.draw_landmarks(img_cv, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
        "#                 fingers = count_fingers(hand_landmarks)\n",
        "\n",
        "#                 if fingers == 1:\n",
        "#                     yolo_emotion_enabled = True\n",
        "#                     tts_enabled = False\n",
        "#                     currency_emotion_enabled = False\n",
        "#                     gesture_text = \"YOLO Emotion Enabled\"\n",
        "\n",
        "#                 elif fingers == 3:\n",
        "#                     yolo_emotion_enabled = False\n",
        "#                     tts_enabled = True\n",
        "#                     currency_emotion_enabled = False\n",
        "#                     gesture_text = \"TTS Enabled\"\n",
        "\n",
        "#                 elif fingers == 5:\n",
        "#                     yolo_emotion_enabled = False\n",
        "#                     currency_emotion_enabled = True\n",
        "#                     tts_enabled = False\n",
        "#                     gesture_text = \"Currency Detection\"\n",
        "\n",
        "#                 cv2.putText(img_cv, f\"Gesture: {gesture_text}\", (50, 100), cv2.FONT_HERSHEY_SIMPLEX,\n",
        "#                             1, (0, 255, 0), 2, cv2.LINE_AA)\n",
        "\n",
        "#         response_data = {\n",
        "#             \"gesture\": gesture_text,\n",
        "#             \"processed_image\": \"\",\n",
        "#             \"audio\": \"\",\n",
        "#         }\n",
        "\n",
        "#         if yolo_emotion_enabled:\n",
        "#             results = yolo_model(img_cv)[0]\n",
        "#             detected_objects = []\n",
        "\n",
        "#             for box in results.boxes:\n",
        "#                 x1, y1, x2, y2 = map(int, box.xyxy[0])\n",
        "#                 cls = int(box.cls[0])\n",
        "#                 label = yolo_model.names[cls]\n",
        "#                 detected_objects.append(label)\n",
        "\n",
        "#             emotion_result = DeepFace.analyze(img_cv, actions=['emotion'], enforce_detection=False, detector_backend='opencv')[0]\n",
        "#             emotion_text = emotion_result['dominant_emotion']\n",
        "#             speech_text = f\"Emotion detected: {emotion_text}. Objects: {', '.join(detected_objects)}.\"\n",
        "\n",
        "#             current_time = time.time()\n",
        "#             if current_time - last_spoken_time >= 15:\n",
        "#                 last_spoken_time = current_time\n",
        "#                 response_data[\"audio\"] = generate_audio(speech_text)\n",
        "\n",
        "#         elif currency_emotion_enabled:\n",
        "#             results = currency_model(img_cv)[0]\n",
        "#             detected_objects = []\n",
        "\n",
        "#             for box in results.boxes:\n",
        "#                 x1, y1, x2, y2 = map(int, box.xyxy[0])\n",
        "#                 cls = int(box.cls[0])\n",
        "#                 label = currency_model.names[cls]  # Use the correct model's class names\n",
        "\n",
        "#                 # Replace \"Rs.\" with \"Rupees\" for better TTS compatibility\n",
        "#                 label = label.replace(\"Rs.\", \"Rupees\")\n",
        "#                 detected_objects.append(label)\n",
        "\n",
        "#             emotion_result = DeepFace.analyze(img_cv, actions=['emotion'], enforce_detection=False, detector_backend='opencv')[0]\n",
        "#             #emotion_text = emotion_result['dominant_emotion']\n",
        "#             speech_text = f\" Objects: {', '.join(detected_objects)}.\"\n",
        "\n",
        "#             current_time = time.time()\n",
        "#             if current_time - last_spoken_time >= 15:\n",
        "#                 last_spoken_time = current_time\n",
        "#                 response_data[\"audio\"] = generate_audio(speech_text)\n",
        "\n",
        "#         elif tts_enabled:\n",
        "#             image_path = \"temp_frame.jpg\"\n",
        "#             cv2.imwrite(image_path, img_cv)\n",
        "#             extracted_text, translated_text, processed_image_path = process_image(image_path)\n",
        "\n",
        "#             if translated_text:\n",
        "#                 response_data[\"audio\"] = generate_audio(translated_text)\n",
        "\n",
        "#         _, buffer = cv2.imencode('.jpg', img_cv)\n",
        "#         response_data[\"processed_image\"] = base64.b64encode(buffer).decode('utf-8')\n",
        "\n",
        "#         return jsonify(response_data)\n",
        "\n",
        "#     except Exception as e:\n",
        "#         return jsonify({\"error\": str(e)})\n",
        "\n",
        "\n",
        "# if __name__ == '__main__':\n",
        "#     app.run(port=5000)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from flask import Flask, request, jsonify\n",
        "from flask_cors import CORS\n",
        "import cv2\n",
        "import numpy as np\n",
        "import torch\n",
        "from gtts import gTTS\n",
        "import time\n",
        "from threading import Thread\n",
        "from ultralytics import YOLO\n",
        "from deepface import DeepFace\n",
        "from io import BytesIO\n",
        "from PIL import Image\n",
        "import base64\n",
        "from pyngrok import ngrok\n",
        "import tempfile\n",
        "import mediapipe as mp\n",
        "import pytesseract\n",
        "from langdetect import detect\n",
        "from googletrans import Translator\n",
        "import os\n",
        "import random\n",
        "import google.generativeai as genai\n",
        "import re\n",
        "from gtts import gTTS\n",
        "from IPython.display import Audio, display\n",
        "import tempfile\n",
        "\n",
        "# Initialize Flask app\n",
        "app = Flask(__name__)\n",
        "CORS(app)\n",
        "\n",
        "# Load models\n",
        "\n",
        "pytesseract.pytesseract.tesseract_cmd = r\"/usr/bin/tesseract\"\n",
        "yolo_model = YOLO(\"yolov8n.pt\")  # General object detection\n",
        "currency_model = YOLO(\"/content/drive/MyDrive/best.pt\")  # Custom-trained currency detection\n",
        "\n",
        "# Initialize MediaPipe Hands\n",
        "mp_hands = mp.solutions.hands\n",
        "hands = mp_hands.Hands(min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
        "mp_draw = mp.solutions.drawing_utils\n",
        "\n",
        "# Start ngrok\n",
        "ngrok_tunnel = ngrok.connect(5000)\n",
        "print(f\"Public URL: {ngrok_tunnel.public_url}\")\n",
        "\n",
        "# Track last spoken time\n",
        "last_spoken_time = time.time()\n",
        "\n",
        "# Control flags\n",
        "yolo_emotion_enabled = False\n",
        "tts_enabled = False\n",
        "\n",
        "def count_fingers(hand_landmarks):\n",
        "    \"\"\"Count the number of extended fingers.\"\"\"\n",
        "    finger_tips = [4, 8, 12, 16, 20]\n",
        "    extended_fingers = sum(\n",
        "        1 for tip in finger_tips if hand_landmarks.landmark[tip].y < hand_landmarks.landmark[tip - 2].y\n",
        "    )\n",
        "    return extended_fingers\n",
        "\n",
        "def generate_audio(text):\n",
        "    \"\"\"Generate speech audio using gTTS and return Base64-encoded MP3.\"\"\"\n",
        "    tts = gTTS(text=text, lang='en')\n",
        "\n",
        "    with tempfile.NamedTemporaryFile(delete=True, suffix=\".mp3\") as temp_audio:\n",
        "        tts.save(temp_audio.name)\n",
        "\n",
        "        # Read and encode the audio file\n",
        "        with open(temp_audio.name, \"rb\") as f:\n",
        "            audio_base64 = base64.b64encode(f.read()).decode(\"utf-8\")\n",
        "\n",
        "    return audio_base64\n",
        "\n",
        "def preprocess_frame(frame):\n",
        "    \"\"\"Preprocess the frame for TTS functionality.\"\"\"\n",
        "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "    upscale_factor = 4\n",
        "    upscaled = cv2.resize(gray, (gray.shape[1] * upscale_factor, gray.shape[0] * upscale_factor))\n",
        "    denoised = cv2.medianBlur(upscaled, 5)\n",
        "    blurred = cv2.GaussianBlur(denoised, (9, 9), 0)\n",
        "    thresh = cv2.adaptiveThreshold(blurred, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 35, 2)\n",
        "    kernel = np.ones((1, 1), np.uint8)\n",
        "    dilated = cv2.dilate(thresh, kernel, iterations=1)\n",
        "    return dilated\n",
        "\n",
        "def process_image(image_path):\n",
        "    \"\"\"Process the image and extract text for TTS functionality.\"\"\"\n",
        "    try:\n",
        "        img = Image.open(image_path)\n",
        "        api_key = \"AIzaSyBw8BSw6ea6ZdJ9Ck77TXSONiHYO7q4VnI\"\n",
        "        genai.configure(api_key=api_key)\n",
        "\n",
        "# Feed the selected instruction into Gemini\n",
        "        model = genai.GenerativeModel(model_name=\"gemini-1.5-flash\")\n",
        "        response = model.generate_content([\"I will provide you an image, extract its text, translate the respective language to english and produce only that, nothing else\", img])\n",
        "\n",
        "\n",
        "        raw_text_sample = response.text\n",
        "        detected_lang = detect(raw_text_sample) if raw_text_sample else \"unknown\"\n",
        "\n",
        "        extracted_text = response.text\n",
        "        # Sample output from the code\n",
        "\n",
        "\n",
        "# Configure Gemini API\n",
        "\n",
        "\n",
        "# Output Gemini's analysis\n",
        "        gemini_analysis = response.text\n",
        "        if not extracted_text:\n",
        "            return None, None, None\n",
        "\n",
        "        translator = Translator()\n",
        "        translated_text = gemini_analysis\n",
        "        if detected_lang != \"en\":\n",
        "            try:\n",
        "                translation_result = translator.translate(gemini_analysis, src=detected_lang, dest=\"en\")\n",
        "                translated_text = translation_result.text if translation_result else \"Translation failed\"\n",
        "            except Exception as e:\n",
        "                translated_text = \"Translation failed\"\n",
        "\n",
        "        processed_image_path = \"processed_image.jpg\"\n",
        "        img.save(processed_image_path)\n",
        "\n",
        "        return extracted_text, translated_text, processed_image_path\n",
        "\n",
        "    except Exception as e:\n",
        "        return None, None, None\n",
        "\n",
        "\n",
        "@app.route('/detect', methods=['POST'])\n",
        "def detect_objects_and_emotions():\n",
        "    global last_spoken_time, yolo_emotion_enabled, tts_enabled, currency_emotion_enabled\n",
        "\n",
        "\n",
        "    try:\n",
        "        img_data = request.json['image']\n",
        "        img_bytes = base64.b64decode(img_data)\n",
        "        img = Image.open(BytesIO(img_bytes)).convert(\"RGB\")\n",
        "\n",
        "        img_np = np.array(img)\n",
        "        img_cv = cv2.cvtColor(img_np, cv2.COLOR_RGB2BGR)\n",
        "\n",
        "        rgb_img = cv2.cvtColor(img_cv, cv2.COLOR_BGR2RGB)\n",
        "        result = hands.process(rgb_img)\n",
        "\n",
        "        gesture_text = \"No Change\"\n",
        "        if result.multi_hand_landmarks:\n",
        "            for hand_landmarks in result.multi_hand_landmarks:\n",
        "                mp_draw.draw_landmarks(img_cv, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
        "                fingers = count_fingers(hand_landmarks)\n",
        "\n",
        "                if fingers == 1:\n",
        "                    yolo_emotion_enabled = True\n",
        "                    tts_enabled = False\n",
        "                    currency_emotion_enabled = False\n",
        "                    gesture_text = \"YOLO Emotion Enabled\"\n",
        "\n",
        "                elif fingers == 3:\n",
        "                    yolo_emotion_enabled = False\n",
        "                    tts_enabled = True\n",
        "                    currency_emotion_enabled = False\n",
        "                    gesture_text = \"TTS Enabled\"\n",
        "\n",
        "                elif fingers == 5:\n",
        "                    yolo_emotion_enabled = False\n",
        "                    currency_emotion_enabled = True\n",
        "                    tts_enabled = False\n",
        "                    gesture_text = \"Currency Detection\"\n",
        "\n",
        "                cv2.putText(img_cv, f\"Gesture: {gesture_text}\", (50, 100), cv2.FONT_HERSHEY_SIMPLEX,\n",
        "                            1, (0, 255, 0), 2, cv2.LINE_AA)\n",
        "\n",
        "        response_data = {\n",
        "            \"gesture\": gesture_text,\n",
        "            \"processed_image\": \"\",\n",
        "            \"audio\": \"\",\n",
        "        }\n",
        "\n",
        "        # if yolo_emotion_enabled:\n",
        "        #     results = yolo_model(img_cv)[0]\n",
        "        #     detected_objects = []\n",
        "\n",
        "        #     for box in results.boxes:\n",
        "        #         x1, y1, x2, y2 = map(int, box.xyxy[0])\n",
        "        #         cls = int(box.cls[0])\n",
        "        #         label = yolo_model.names[cls]\n",
        "        #         detected_objects.append(label)\n",
        "\n",
        "        #     emotion_result = DeepFace.analyze(img_cv, actions=['emotion'], enforce_detection=False, detector_backend='opencv')[0]\n",
        "        #     emotion_text = emotion_result['dominant_emotion']\n",
        "\n",
        "        #     speech_text = f\"Emotion detected: {emotion_text}. Objects: {', '.join(detected_objects)}.\"\n",
        "\n",
        "        #     current_time = time.time()\n",
        "        #     if current_time - last_spoken_time >= 15:\n",
        "        #         last_spoken_time = current_time\n",
        "        #         response_data[\"audio\"] = generate_audio(speech_text)\n",
        "\n",
        "        IMPORTANT_OBJECTS = {2: \"car\", 3: \"motorcycle\", 5: \"bus\", 7: \"truck\"}\n",
        "\n",
        "        if yolo_emotion_enabled:\n",
        "            results = yolo_model(img_cv)[0]\n",
        "            detected_objects = []\n",
        "            vehicle_detected = False\n",
        "            vehicle_labels = []\n",
        "\n",
        "            for box in results.boxes:\n",
        "                x1, y1, x2, y2 = map(int, box.xyxy[0])\n",
        "                cls = int(box.cls[0])\n",
        "                label = yolo_model.names[cls]\n",
        "                detected_objects.append(label)\n",
        "\n",
        "                if cls in IMPORTANT_OBJECTS:\n",
        "                    vehicle_detected = True\n",
        "                    vehicle_labels.append(IMPORTANT_OBJECTS[cls])\n",
        "\n",
        "            # Construct the base speech text\n",
        "            speech_text = f\"Objects: {', '.join(detected_objects)}.\"\n",
        "\n",
        "            # Add emotion detection if person is present\n",
        "            if 'person' in detected_objects:\n",
        "                emotion_result = DeepFace.analyze(img_cv, actions=['emotion'], enforce_detection=False, detector_backend='opencv')[0]\n",
        "                emotion_text = emotion_result['dominant_emotion']\n",
        "                speech_text = f\"Emotion detected: {emotion_text}. {speech_text}\"\n",
        "\n",
        "            # Add vehicle alert if any vehicle is detected\n",
        "            if vehicle_detected:\n",
        "                vehicle_set = set(vehicle_labels)  # Remove duplicates\n",
        "                vehicle_text = f\"Warning! {', '.join(vehicle_set)} detected nearby.\"\n",
        "                speech_text = f\"{vehicle_text} {speech_text}\"\n",
        "\n",
        "            # Speak only if 15 seconds have passed\n",
        "            current_time = time.time()\n",
        "            if current_time - last_spoken_time >= 15:\n",
        "                last_spoken_time = current_time\n",
        "                response_data[\"audio\"] = generate_audio(speech_text)\n",
        "\n",
        "\n",
        "        elif currency_emotion_enabled:\n",
        "            results = currency_model(img_cv)[0]\n",
        "            detected_objects = []\n",
        "\n",
        "            for box in results.boxes:\n",
        "                x1, y1, x2, y2 = map(int, box.xyxy[0])\n",
        "                cls = int(box.cls[0])\n",
        "                label = currency_model.names[cls]  # Use the correct model's class names\n",
        "\n",
        "                # Replace \"Rs.\" with \"Rupees\" for better TTS compatibility\n",
        "                label = label.replace(\"Rs.\", \"Rupees\")\n",
        "                detected_objects.append(label)\n",
        "\n",
        "            emotion_result = DeepFace.analyze(img_cv, actions=['emotion'], enforce_detection=False, detector_backend='opencv')[0]\n",
        "            #emotion_text = emotion_result['dominant_emotion']\n",
        "            speech_text = f\" Objects: {', '.join(detected_objects)}.\"\n",
        "\n",
        "            current_time = time.time()\n",
        "            if current_time - last_spoken_time >= 15:\n",
        "                last_spoken_time = current_time\n",
        "                response_data[\"audio\"] = generate_audio(speech_text)\n",
        "\n",
        "        elif tts_enabled:\n",
        "            image_path = \"temp_frame.jpg\"\n",
        "            cv2.imwrite(image_path, img_cv)\n",
        "            extracted_text, translated_text, processed_image_path = process_image(image_path)\n",
        "\n",
        "            if translated_text:\n",
        "                response_data[\"audio\"] = generate_audio(translated_text)\n",
        "\n",
        "        _, buffer = cv2.imencode('.jpg', img_cv)\n",
        "        response_data[\"processed_image\"] = base64.b64encode(buffer).decode('utf-8')\n",
        "\n",
        "        return jsonify(response_data)\n",
        "\n",
        "    except Exception as e:\n",
        "        return jsonify({\"error\": str(e)})\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    app.run(port=5000)\n"
      ],
      "metadata": {
        "id": "Rwe7wZRs29FJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from flask import Flask, request, jsonify\n",
        "from flask_cors import CORS\n",
        "import cv2\n",
        "import numpy as np\n",
        "import torch\n",
        "from gtts import gTTS\n",
        "import time\n",
        "from threading import Thread\n",
        "from ultralytics import YOLO\n",
        "from deepface import DeepFace\n",
        "from io import BytesIO\n",
        "from PIL import Image\n",
        "import base64\n",
        "from pyngrok import ngrok\n",
        "import tempfile\n",
        "import mediapipe as mp\n",
        "import pytesseract\n",
        "from langdetect import detect\n",
        "from googletrans import Translator\n",
        "import os\n",
        "import random\n",
        "import google.generativeai as genai\n",
        "import re\n",
        "from gtts import gTTS\n",
        "from IPython.display import Audio, display\n",
        "import tempfile\n",
        "\n",
        "# Initialize Flask app\n",
        "app = Flask(__name__)\n",
        "CORS(app)\n",
        "\n",
        "# Load models\n",
        "\n",
        "pytesseract.pytesseract.tesseract_cmd = r\"/usr/bin/tesseract\"\n",
        "yolo_model = YOLO(\"yolov8n.pt\")  # General object detection\n",
        "currency_model = YOLO(\"/content/drive/MyDrive/best.pt\")  # Custom-trained currency detection\n",
        "\n",
        "# Initialize MediaPipe Hands\n",
        "mp_hands = mp.solutions.hands\n",
        "hands = mp_hands.Hands(min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
        "mp_draw = mp.solutions.drawing_utils\n",
        "\n",
        "# Start ngrok\n",
        "ngrok_tunnel = ngrok.connect(5000)\n",
        "print(f\"Public URL: {ngrok_tunnel.public_url}\")\n",
        "\n",
        "# Track last spoken time\n",
        "last_spoken_time = time.time()\n",
        "\n",
        "# Control flags\n",
        "yolo_emotion_enabled = False\n",
        "tts_enabled = False\n",
        "\n",
        "def count_fingers(hand_landmarks):\n",
        "    \"\"\"Count the number of extended fingers.\"\"\"\n",
        "    finger_tips = [4, 8, 12, 16, 20]\n",
        "    extended_fingers = sum(\n",
        "        1 for tip in finger_tips if hand_landmarks.landmark[tip].y < hand_landmarks.landmark[tip - 2].y\n",
        "    )\n",
        "    return extended_fingers\n",
        "\n",
        "def generate_audio(text):\n",
        "    \"\"\"Generate speech audio using gTTS and return Base64-encoded MP3.\"\"\"\n",
        "    tts = gTTS(text=text, lang='en')\n",
        "\n",
        "    with tempfile.NamedTemporaryFile(delete=True, suffix=\".mp3\") as temp_audio:\n",
        "        tts.save(temp_audio.name)\n",
        "\n",
        "        # Read and encode the audio file\n",
        "        with open(temp_audio.name, \"rb\") as f:\n",
        "            audio_base64 = base64.b64encode(f.read()).decode(\"utf-8\")\n",
        "\n",
        "    return audio_base64\n",
        "\n",
        "def preprocess_frame(frame):\n",
        "    \"\"\"Preprocess the frame for TTS functionality.\"\"\"\n",
        "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "    upscale_factor = 4\n",
        "    upscaled = cv2.resize(gray, (gray.shape[1] * upscale_factor, gray.shape[0] * upscale_factor))\n",
        "    denoised = cv2.medianBlur(upscaled, 5)\n",
        "    blurred = cv2.GaussianBlur(denoised, (9, 9), 0)\n",
        "    thresh = cv2.adaptiveThreshold(blurred, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 35, 2)\n",
        "    kernel = np.ones((1, 1), np.uint8)\n",
        "    dilated = cv2.dilate(thresh, kernel, iterations=1)\n",
        "    return dilated\n",
        "\n",
        "def process_image(image_path):\n",
        "    \"\"\"Process the image and extract text for TTS functionality.\"\"\"\n",
        "    try:\n",
        "        img = Image.open(image_path)\n",
        "        api_key = \"AIzaSyBw8BSw6ea6ZdJ9Ck77TXSONiHYO7q4VnI\"\n",
        "        genai.configure(api_key=api_key)\n",
        "\n",
        "# Feed the selected instruction into Gemini\n",
        "        model = genai.GenerativeModel(model_name=\"gemini-1.5-flash\")\n",
        "        response = model.generate_content([\"I will provide you an image, extract its text, translate the respective language to english and produce only that, nothing else\", img])\n",
        "\n",
        "\n",
        "        raw_text_sample = response.text\n",
        "        detected_lang = detect(raw_text_sample) if raw_text_sample else \"unknown\"\n",
        "\n",
        "        extracted_text = response.text\n",
        "        # Sample output from the code\n",
        "\n",
        "\n",
        "# Configure Gemini API\n",
        "\n",
        "\n",
        "# Output Gemini's analysis\n",
        "        gemini_analysis = response.text\n",
        "        if not extracted_text:\n",
        "            return None, None, None\n",
        "\n",
        "        translator = Translator()\n",
        "        translated_text = gemini_analysis\n",
        "        if detected_lang != \"en\":\n",
        "            try:\n",
        "                translation_result = translator.translate(gemini_analysis, src=detected_lang, dest=\"en\")\n",
        "                translated_text = translation_result.text if translation_result else \"Translation failed\"\n",
        "            except Exception as e:\n",
        "                translated_text = \"Translation failed\"\n",
        "\n",
        "        processed_image_path = \"processed_image.jpg\"\n",
        "        img.save(processed_image_path)\n",
        "\n",
        "        return extracted_text, translated_text, processed_image_path\n",
        "\n",
        "    except Exception as e:\n",
        "        return None, None, None\n",
        "\n",
        "\n",
        "@app.route('/detect', methods=['POST'])\n",
        "def detect_objects_and_emotions():\n",
        "    global last_spoken_time, yolo_emotion_enabled, tts_enabled, currency_emotion_enabled\n",
        "\n",
        "\n",
        "    try:\n",
        "        img_data = request.json['image']\n",
        "        img_bytes = base64.b64decode(img_data)\n",
        "        img = Image.open(BytesIO(img_bytes)).convert(\"RGB\")\n",
        "\n",
        "        img_np = np.array(img)\n",
        "        img_cv = cv2.cvtColor(img_np, cv2.COLOR_RGB2BGR)\n",
        "\n",
        "        rgb_img = cv2.cvtColor(img_cv, cv2.COLOR_BGR2RGB)\n",
        "        result = hands.process(rgb_img)\n",
        "\n",
        "        gesture_text = \"No Change\"\n",
        "        if result.multi_hand_landmarks:\n",
        "            for hand_landmarks in result.multi_hand_landmarks:\n",
        "                mp_draw.draw_landmarks(img_cv, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
        "                fingers = count_fingers(hand_landmarks)\n",
        "\n",
        "                if fingers == 1:\n",
        "                    yolo_emotion_enabled = True\n",
        "                    tts_enabled = False\n",
        "                    currency_emotion_enabled = False\n",
        "                    gesture_text = \"YOLO Emotion Enabled\"\n",
        "\n",
        "                elif fingers == 3:\n",
        "                    yolo_emotion_enabled = False\n",
        "                    tts_enabled = True\n",
        "                    currency_emotion_enabled = False\n",
        "                    gesture_text = \"TTS Enabled\"\n",
        "\n",
        "                elif fingers == 5:\n",
        "                    yolo_emotion_enabled = False\n",
        "                    currency_emotion_enabled = True\n",
        "                    tts_enabled = False\n",
        "                    gesture_text = \"Currency Detection\"\n",
        "\n",
        "                cv2.putText(img_cv, f\"Gesture: {gesture_text}\", (50, 100), cv2.FONT_HERSHEY_SIMPLEX,\n",
        "                            1, (0, 255, 0), 2, cv2.LINE_AA)\n",
        "\n",
        "        response_data = {\n",
        "            \"gesture\": gesture_text,\n",
        "            \"processed_image\": \"\",\n",
        "            \"audio\": \"\",\n",
        "        }\n",
        "\n",
        "        # if yolo_emotion_enabled:\n",
        "        #     results = yolo_model(img_cv)[0]\n",
        "        #     detected_objects = []\n",
        "\n",
        "        #     for box in results.boxes:\n",
        "        #         x1, y1, x2, y2 = map(int, box.xyxy[0])\n",
        "        #         cls = int(box.cls[0])\n",
        "        #         label = yolo_model.names[cls]\n",
        "        #         detected_objects.append(label)\n",
        "\n",
        "        #     emotion_result = DeepFace.analyze(img_cv, actions=['emotion'], enforce_detection=False, detector_backend='opencv')[0]\n",
        "        #     emotion_text = emotion_result['dominant_emotion']\n",
        "\n",
        "        #     speech_text = f\"Emotion detected: {emotion_text}. Objects: {', '.join(detected_objects)}.\"\n",
        "\n",
        "        #     current_time = time.time()\n",
        "        #     if current_time - last_spoken_time >= 15:\n",
        "        #         last_spoken_time = current_time\n",
        "        #         response_data[\"audio\"] = generate_audio(speech_text)\n",
        "\n",
        "        if yolo_emotion_enabled:\n",
        "            results = yolo_model(img_cv)[0]\n",
        "            detected_objects = []\n",
        "\n",
        "            for box in results.boxes:\n",
        "              x1, y1, x2, y2 = map(int, box.xyxy[0])\n",
        "              cls = int(box.cls[0])\n",
        "              label = yolo_model.names[cls]\n",
        "              detected_objects.append(label)\n",
        "\n",
        "            speech_text = f\"Objects: {', '.join(detected_objects)}.\"\n",
        "\n",
        "            if 'person' in detected_objects:\n",
        "              emotion_result = DeepFace.analyze(img_cv, actions=['emotion'], enforce_detection=False, detector_backend='opencv')[0]\n",
        "              emotion_text = emotion_result['dominant_emotion']\n",
        "              speech_text = f\"Emotion detected: {emotion_text}. {speech_text}\"\n",
        "\n",
        "            current_time = time.time()\n",
        "            if current_time - last_spoken_time >= 15:\n",
        "              last_spoken_time = current_time\n",
        "              response_data[\"audio\"] = generate_audio(speech_text)\n",
        "\n",
        "\n",
        "        elif currency_emotion_enabled:\n",
        "            results = currency_model(img_cv)[0]\n",
        "            detected_objects = []\n",
        "\n",
        "            for box in results.boxes:\n",
        "                x1, y1, x2, y2 = map(int, box.xyxy[0])\n",
        "                cls = int(box.cls[0])\n",
        "                label = currency_model.names[cls]  # Use the correct model's class names\n",
        "\n",
        "                # Replace \"Rs.\" with \"Rupees\" for better TTS compatibility\n",
        "                label = label.replace(\"Rs.\", \"Rupees\")\n",
        "                detected_objects.append(label)\n",
        "\n",
        "            emotion_result = DeepFace.analyze(img_cv, actions=['emotion'], enforce_detection=False, detector_backend='opencv')[0]\n",
        "            #emotion_text = emotion_result['dominant_emotion']\n",
        "            speech_text = f\" Objects: {', '.join(detected_objects)}.\"\n",
        "\n",
        "            current_time = time.time()\n",
        "            if current_time - last_spoken_time >= 15:\n",
        "                last_spoken_time = current_time\n",
        "                response_data[\"audio\"] = generate_audio(speech_text)\n",
        "\n",
        "        elif tts_enabled:\n",
        "            image_path = \"temp_frame.jpg\"\n",
        "            cv2.imwrite(image_path, img_cv)\n",
        "            extracted_text, translated_text, processed_image_path = process_image(image_path)\n",
        "\n",
        "            if translated_text:\n",
        "                response_data[\"audio\"] = generate_audio(translated_text)\n",
        "\n",
        "        _, buffer = cv2.imencode('.jpg', img_cv)\n",
        "        response_data[\"processed_image\"] = base64.b64encode(buffer).decode('utf-8')\n",
        "\n",
        "        return jsonify(response_data)\n",
        "\n",
        "    except Exception as e:\n",
        "        return jsonify({\"error\": str(e)})\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    app.run(port=5000)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        },
        "id": "d34i9R3SfYTR",
        "outputId": "1b4ffc78-13bb-4c4d-8bc9-62f17151a897"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolov8n.pt to 'yolov8n.pt'...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 6.25M/6.25M [00:00<00:00, 70.3MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Public URL: https://a90b-34-139-133-242.ngrok-free.app\n",
            " * Serving Flask app '__main__'\n",
            " * Debug mode: off\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
            " * Running on http://127.0.0.1:5000\n",
            "INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n",
            "INFO:werkzeug:127.0.0.1 - - [15/Apr/2025 07:14:41] \"POST /detect HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [15/Apr/2025 07:14:43] \"POST /detect HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [15/Apr/2025 07:14:45] \"POST /detect HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [15/Apr/2025 07:14:52] \"POST /detect HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [15/Apr/2025 07:15:00] \"POST /detect HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [15/Apr/2025 07:15:10] \"POST /detect HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [15/Apr/2025 07:15:17] \"POST /detect HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [15/Apr/2025 07:15:26] \"POST /detect HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [15/Apr/2025 07:16:01] \"POST /detect HTTP/1.1\" 200 -\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}